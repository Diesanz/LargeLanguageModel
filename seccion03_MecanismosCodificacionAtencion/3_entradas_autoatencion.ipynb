{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b848eca",
   "metadata": {},
   "source": [
    "# Atender a diferentes partes de la entrada con autoatención\n",
    "\n",
    "Ahora  abordaremos  el  funcionamiento  interno  del  mecanismo  de  autoatención  y  aprenderemos  a  codificarlo  desde  cero.  La  autoatención  es  la  piedra  angular  de  todo  LLM  basado  en  la  arquitectura  del  transformador. \n",
    "\n",
    "- En  la  autoatención,  el  \"yo\"  se  refiere  a  la  capacidad  del  mecanismo  para  calcular  ponderaciones  de  atención  relacionando  diferentes  posiciones  dentro  de  una  sola  secuencia  de  entrada.  Evalúa  y  aprende  las  relaciones  y  dependencias  entre  diversas  partes  de  la  propia  entrada,  como  las  palabras  en  una  oración  o  los  píxeles  en  una  imagen.  Esto  contrasta  con  los  mecanismos  de  atención  tradicionales,  donde  la  atención  se  centra  en  las  relaciones  entre  elementos  de  dos  secuencias  diferentes,  como  en  los  modelos  secuencia  a  secuencia,  donde  la  atención  podría  estar  entre  una  secuencia  de  entrada  y  una  secuencia  de  salida,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fe0e5",
   "metadata": {},
   "source": [
    "### Un mecanismo simple de autoatención sin pesos entrenables\n",
    "\n",
    "El  objetivo  de  esta  sección  es  ilustrar  algunos  conceptos  clave  de  la  autoatención  antes  de  añadir  pesos  entrenables\n",
    "\n",
    "![Texto alternativo](./imgs/3.7.png)\n",
    "\n",
    "\n",
    "El objetivo de la **autoatención** es calcular, para cada elemento de entrada de una secuencia, un **vector de contexto** que combine información de todos los demás elementos de esa secuencia.  \n",
    "\n",
    "- La figura muestra una secuencia de entrada \\(x\\), formada por \\(T\\) elementos, representados como $x^{(1)}, x^{(2)}, \\dots, x^{(T)}$.  \n",
    "- Esta secuencia normalmente corresponde a texto (por ejemplo, una oración) que ya ha sido convertido en **vectores de incrustación** (embeddings de tokens).  \n",
    "  - Ejemplo: si el texto de entrada es *“Tu viaje comienza con un paso”*, cada token (“Tu”, “viaje”, “comienza”, …) se convierte en un vector de incrustación de dimensión \\(d\\).  \n",
    "- En la figura, los vectores de entrada se muestran como incrustaciones tridimensionales para simplificar la visualización.\n",
    "\n",
    "En la autoatención, lo que buscamos es calcular, para cada $x^{(i)}$, un **vector de contexto $z^{(i)}$**.  \n",
    "- Este vector de contexto es un embedding enriquecido, ya que no solo contiene información sobre el token $x^{(i)}$ en sí, sino también sobre todos los demás tokens de la secuencia $x^{(1)}, \\dots, x^{(T)}$.  \n",
    "- Para lograrlo, se usan los **pesos de atención**, que indican la importancia relativa de cada elemento de entrada en el cálculo del contexto de un token concreto.  \n",
    "\n",
    "Por ejemplo:  \n",
    "- Si nos fijamos en el segundo token $x^{(2)} =$ “viaje”, el vector de contexto correspondiente es $z^{(2)}$.  \n",
    "- Este $z^{(2)}$ combina información tanto de “viaje” como del resto de tokens (“Tu”, “comienza”, “con”, …).  \n",
    "- Así, $z^{(2)}$ es una representación más rica y útil que la incrustación inicial de “viaje” aislada.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afbb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Considerar  la  siguiente  oración  de  entrada,  que  ya  ha  sido  incorporada  en  vectores  tridimensionales \n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1546316",
   "metadata": {},
   "source": [
    "El  primer  paso  para  implementar  la  autoatención  es  calcular  los  valores  intermedios  ω,  denominados  puntuaciones  de  atención.\n",
    "\n",
    "![Texto alternativo](./imgs/3.8.png)\n",
    "\n",
    "1. **Consulta (query)**  \n",
    "   - El segundo token de entrada, $x^{(2)}$, se considera la **consulta**.  \n",
    "   - La consulta se compara con todos los demás tokens de la secuencia para determinar su importancia relativa.\n",
    "\n",
    "2. **Cálculo de puntuaciones de atención**  \n",
    "   - Se calculan como un **producto escalar** entre la consulta y cada token de la entrada:  \n",
    "     $\n",
    "     \\text{score}(x^{(2)}, x^{(j)}) = x_{\\text{consulta}} \\cdot x^{(j)}\n",
    "     $\n",
    "   - Aquí, $x_{\\text{consulta}} = x^{(2)}$.\n",
    "\n",
    "3. **Normalización y truncado**  \n",
    "   - Las cifras pueden truncarse a un dígito para simplificar la visualización.  \n",
    "   - Después de calcular los productos escalares, se aplica **softmax** para obtener los **pesos de atención**.\n",
    "\n",
    "4. **Vector de contexto**  \n",
    "   - Finalmente, el vector de contexto $z^{(2)}$ se obtiene como la **combinación ponderada de todos los tokens de entrada**, usando los pesos de atención:\n",
    "     $$\n",
    "     z^{(2)} = \\sum_{j=1}^{T} \\text{peso\\_atención}_j \\cdot x^{(j)}\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084c8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "att_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    att_scores_2[i] = torch.dot(query, x_i)\n",
    "print(att_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0a9e9",
   "metadata": {},
   "source": [
    "Normalizar los pesos de atención.\n",
    "\n",
    "![Texto alternativo](./imgs/3.9.png)\n",
    "\n",
    "El objetivo es obtener pesos de atención que sumen 1. Esta normalización es útil para la interpretación y para mantener la estabilidad del entrenamiento en un LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339088c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de atencion:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Suma:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "att_weights_2_tmp = att_scores_2 / att_scores_2.sum()\n",
    "print(\"Pesos de atencion: \", att_weights_2_tmp)\n",
    "print(\"Suma: \", att_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e9e84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de atencion:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Suma:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#Más recomendable utilizar la función softmax\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "att_weights_2_naive = softmax(att_scores_2)\n",
    "print(\"Pesos de atencion: \", att_weights_2_naive)\n",
    "print(\"Suma: \", att_weights_2_naive.sum())\n",
    "\n",
    "#Garantiza que los pesos de atención siempre sean positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4eb1f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de atencion:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Suma:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#Utilizar softmax de torch ya que se tiene en cuenta problemas de inestabilidad, desbordamiento y subdesbordamiento\n",
    "att_weights_2 = torch.softmax(att_scores_2, dim=0)\n",
    "print(\"Pesos de atencion: \", att_weights_2)\n",
    "print(\"Suma: \", att_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16227b26",
   "metadata": {},
   "source": [
    "Ahora que ya hemos calculado los pesos de atención normalizados, podemos realizar el paso final para obtener el vector de contexto \n",
    "\n",
    "## Cómo se hace\n",
    "\n",
    "1. **Multiplicar cada embedding por su peso de atención**  \n",
    "   - Cada token de entrada $x^{(j)}$ se multiplica por su peso de atención $\\alpha_j$:\n",
    "     $$\n",
    "     x^{(j)} \\times \\alpha_j\n",
    "     $$\n",
    "   - Esto pondera cada token según su importancia para la consulta.\n",
    "\n",
    "2. **Sumar los vectores ponderados**  \n",
    "   - Después de multiplicar, se suman todos los vectores para formar el **vector de contexto final $z^{(i)}$**:\n",
    "     $$\n",
    "     z^{(i)} = \\sum_{j=1}^{T} \\alpha_j \\, x^{(j)}\n",
    "     $$\n",
    "   - Aquí, $T$ es el número total de tokens en la secuencia.\n",
    "\n",
    "- $z^{(i)}$ es una **representación enriquecida** de $x^{(i)}$, incorporando información de todos los demás tokens de la secuencia, ponderada por su relevancia.  \n",
    "- Cada token “mira” a todos los demás y recoge la información más relevante para entender su contexto.\n",
    "\n",
    "![Texto alternativo](./imgs/3.10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373a55d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] #2nd input es la query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += att_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5a13c",
   "metadata": {},
   "source": [
    "## Cálculo de pesos de atención para todos los tokens de entrada\n",
    "\n",
    "Se han calculamos  los  pesos  de  atención  y  el  vector  de  contexto  para  la  entrada  2, ahora   ampliar este  cálculo  para  calcular  los  pesos  de  atención  y  los  vectores  de  contexto  para  todas  las  entradas\n",
    "\n",
    "![Texto alternativo](./imgs/3.11.png)\n",
    "\n",
    "En la imagen se muestra de forma resaltada los pesos de atención respecto al segundo elemento.\n",
    "\n",
    "Para lograr estos solo hay que seguir los mismos pasos anteriormente descrito, pero de forma generalizada.\n",
    "\n",
    "1 - Calcular puntajes de atención \n",
    "\n",
    "2 - Calcular pesos de atención --> Normalización \n",
    "\n",
    "3 - Calcular vectores de contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fabdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#1. Calcular los puntuajes de atencion\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, i_x in enumerate(inputs):\n",
    "    for j, j_y in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(i_x, j_y)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ad0d3",
   "metadata": {},
   "source": [
    "Cada  elemento  del  tensor  anterior  representa  una  puntuación  de  atención  entre  cada  par  de entradas.\n",
    "\n",
    "Debido a que los bucles for son lentos, se pueden obtener los mismos resultados con multiplicación de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cb4802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "848e1c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "#2. Calcular los pesos de atencion\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)\n",
    "\n",
    "#Al establecer dim=1, se indica a softmax que la normalización sea a lo largo de las columnas, \n",
    "#ya que el tensor es 2D de la forma [filas, columnas].\n",
    "#Por lo tanto la suma de cada columnas debe ser 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77a439e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificar que la suma de las filas es uno\n",
    "attn_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "715ea9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "#3. Calculas los vectores de contexto \n",
    "attn_context_vectors = attn_weights @ inputs\n",
    "print(attn_context_vectors)\n",
    "print(attn_weights.shape)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0344dd9",
   "metadata": {},
   "source": [
    "\n",
    "[Implemenatación de la autoatención con pesos entrenables](./4_autoatencion_pesos_entrenables.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
