{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a990ee2",
   "metadata": {},
   "source": [
    "# Ampliación de la atención de una sola cabeza a la atención multicabecera\n",
    "\n",
    "Ampliacion de la clase de atención casual a una de atención multicabezera.\n",
    "\n",
    "El  término  \"multicabezal\"  se  refiere  a  la  división  del  mecanismo  de  atención  en  múltiples  \"cabezas\",  cada  una  operando  de  forma  independiente.  En  este  contexto,  un  único  módulo  de  atención  causal  puede  considerarse  atención  de  cabeza  única,  donde  solo  hay  un  conjunto  de  ponderaciones  de  atención  que  procesan  la  entrada  secuencialmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9ddec",
   "metadata": {},
   "source": [
    "## Ailamiento de múltiples capas de atención de un solo cabezal\n",
    "\n",
    "En  términos  prácticos,  implementar  la  atención  de  múltiples  cabezas  implica  crear  múltiples  instancias del  mecanismo  de  autoatención cada uno con sus pesos y luego combinar sus resultados.  Usar  múltiples  instancias  del  mecanismo  de  autoatención  puede  ser  computacionalmente  intensivo,  pero  es  crucial  para  el  tipo  de reconocimiento  de  patrones  complejos  por  los  que  son  conocidos  modelos  como  los  LLM  basados  en  transformadores.\n",
    "\n",
    "\n",
    "![Texto alternativo](./imgs/3.23.png)\n",
    "\n",
    "- En **atención de una sola cabeza** usas una sola matriz \\( W_v \\) para calcular los valores → un único vector de contexto \\( Z \\).\n",
    "\n",
    "- En **atención de múltiples cabezas (multi-head)**, se usan varias matrices \\( W_{v1}, W_{v2}, \\dots \\) (y lo mismo para \\( W_q, W_k \\)).\n",
    "\n",
    "- Así, obtienes varios vectores de contexto (\\( Z_1, Z_2, \\dots \\)) que luego se concatenan para formar un vector final.\n",
    "\n",
    "La  idea  principal  detrás  de  la  atención  de  múltiples  cabezas  es  ejecutar  el  mecanismo  de  atención  varias  veces  (en  paralelo)  con  diferentes  proyecciones  lineales  aprendidas:  los  resultados  de  multiplicar  los  datos  de  entrada  (como  los  vectores  de  consulta,  clave  y  valor  en  los  \n",
    "mecanismos  de  atención)  por  una  matriz  de  peso.\n",
    "En  el  código,  podemos  lograr  esto  implementando  una  clase  MultiHeadAttentionWrapper  simple  que  apila  múltiples  instancias  de  nuestro  CausalAttention  implementado  previamente.módulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6eb6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout,  qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  #Capa de abandono\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                            \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)              #transponer las dimensiones 1 y 2,manteniendo la dimension del lote en la primera dimension (0) \n",
    "        attn_scores.masked_fill_(                                 #las operaciones con guion dinal sen realiza en el lugar lo que evita copias de meoria inecesarias\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a331536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiltiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CasualAttention(d_in, d_out, context_length, dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fceecc",
   "metadata": {},
   "source": [
    "Por  ejemplo,  si  utilizamos  esta  clase  MultiHeadAttentionWrapper  con  dos  cabezas  de  atención  (a  través  de  num_heads=2)  y  la  dimensión  de  salida  CausalAttention  d_out=2,  esto  da  como  resultado  vectores  de  contexto  de  4  dimensiones  (d_out*num_heads=4)\n",
    "\n",
    "\n",
    "![Texto alternativo](./imgs/3.24.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6db095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "mha = MiltiHeadAttentionWrapper(d_in, 2, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49bd5c",
   "metadata": {},
   "source": [
    "## Implementación de la atención multicabecera con división de peso\n",
    "\n",
    "Se usa MultiHeadAttentionWrapper  para  implementar  la  atención  multicabezal  mediante  la  superposición  de  varios  módulos  de  atención  de  un  solo  cabezal.  Esto  se  \n",
    "logró  instanciando  y  combinando  varios  objetos  CausalAttention .\n",
    "\n",
    "En  lugar  de  mantener  dos  clases  separadas,  MultiHeadAttentionWrapper  y  CausalAttention,  podemos  \n",
    "combinar  ambos conceptos en uno solo. \n",
    "\n",
    "En  MultiHeadAttentionWrapper,  se  implementan  múltiples  cabezales  mediante  la  creación  de  una  lista  de  objetos  CausalAttention  (self.heads),  cada  uno  de  los  cuales  representa  un  cabezal  de  atención  independiente. La  clase  CausalAttention  ejecuta  el  mecanismo  de  atención  de  forma  independiente,  y  los  resultados  de  cada  \n",
    "encabezado  se  concatenan.  Por  el  contrario,  la  clase  MultiHeadAttention ,  que  se  describe  a  continuación ,  integra  la  funcionalidad  multiencabezado  en  una  sola  clase.  Divide  la  entrada  en  varios  encabezados  mediante  la  remodelación  de  los  tensores  proyectados  de  consulta,  clave  y  valor,  y  luego  combina  los  resultados  de  \n",
    "estos  encabezados  tras  calcular  la  atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9675c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads                        #Reduzca  la  atenuación  de  la  proyección  para  que  coincida  con  la  atenuación  de  salida  deseada\n",
    "        #si d_out=512 y num_heads=8, cada cabeza tendrá dimensión 64\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)                   #Capa lineal para combinar las salidas de la cabeza\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer( #evitar mirar a tokens futuros\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #(2, 6, 3)\n",
    "        keys = self.W_key(x)                                      #(b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)                                 \n",
    "        values = self.W_value(x)                                  \n",
    "        #Dividir  implícitamente  la  matriz  añadiendo  una  dimensión  `num_heads`.  Luego,  desenrollamos  la  última  dimensión:  (b,\n",
    "        #núm_tokens,  d_out)  >  (b,  núm_tokens,  núm_cabezas,  cabeza_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.transpose(1, 2)  #Transponer  de  la  forma  (b,  num_tokens,  num_heads,  head_dim)  a  (b,  num_heads,  num_tokens,  head_dim)                             \n",
    "        queries = queries.transpose(1, 2)                         \n",
    "        values = values.transpose(1, 2)  \n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)            \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]   #Mascara de truncamiento \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)           \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)       \n",
    "        #Combina  cabezas,  donde  self.d_out  =  self.num_heads  *  self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)         #Agregar  una  proyección  lineal  opcional         \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18186d9a",
   "metadata": {},
   "source": [
    "A  nivel  general,  en  el  MultiHeadAttentionWrapper  anterior,  apilamoscomoelmúltiples  capas  de  atención  de  una  sola  cabeza  que  combinamos  en  una  capa  de  atención  de  múltiples  cabezas.La  clase  MultiHeadAttention  adopta  un  enfoque  integrado.  Comienza  con  un  multihead capa  y  luego  divide  internamente  esta  capa  en  cabezas  de  atención  individuales.\n",
    "\n",
    "![Texto alternativo](./imgs/3.25.png)\n",
    "\n",
    "En la atención multi-cabeza, los tensores de consultas (Q), claves (K) y valores (V) primero se generan con capas lineales de dimensión (b, num_tokens, d_out).\n",
    "Luego:\n",
    "\n",
    "1. División de la dimensión de salida\n",
    "\n",
    "- Se reordena con .view en (b, num_tokens, num_heads, head_dim),\n",
    "\n",
    "- donde head_dim = d_out / num_heads.\n",
    "\n",
    "- Esto separa la información en varias cabezas más pequeñas.\n",
    "\n",
    "2. Transposición\n",
    "\n",
    "- Se aplica .transpose para obtener (b, num_heads, num_tokens, head_dim).\n",
    "\n",
    "- Este paso pone la dimensión num_heads delante de num_tokens, lo que facilita los cálculos de atención en paralelo.\n",
    "\n",
    "3. Importancia\n",
    "\n",
    "- La división y transposición permiten alinear correctamente Q, K y V por cabeza y realizar las multiplicaciones de matrices de atención de forma eficiente en lotes (batch matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61be7db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],      # (b,num_heads,num_tokens,head_dim)=(1, 2, 3, 4)       \n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                    [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "print(a@a.transpose(2, 3)) #(3*4) @ (4*3) = (3*3)\n",
    "a.transpose(2, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cab2217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2745, 0.6584, 0.2775, 0.8573],\n",
      "        [0.8993, 0.0390, 0.9268, 0.7388],\n",
      "        [0.7179, 0.7058, 0.9156, 0.4340]])\n",
      "tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "print(first_head)\n",
    "first_res = first_head @ first_head.T\n",
    "print(first_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347fdd0",
   "metadata": {},
   "source": [
    "Después de calcular los pesos de atención y los vectores de contexto:\n",
    "\n",
    "1. Reorganización de los vectores de contexto\n",
    "\n",
    "- Se transponen nuevamente a (b, num_tokens, num_heads, head_dim).\n",
    "\n",
    "- Luego se remodelan (aplanan) a (b, num_tokens, d_out), combinando las salidas de todas las cabezas.\n",
    "\n",
    "2. Proyección de salida\n",
    "\n",
    "- Se aplica una capa lineal self.out_proj tras combinar las cabezas.\n",
    "\n",
    "- Esta capa no es estrictamente necesaria, pero se usa en muchas arquitecturas LLM para mezclar la información final.\n",
    "\n",
    "3. Eficiencia\n",
    "\n",
    "- La implementación de MultiHeadAttention es más eficiente que la de MultiHeadAttentionWrapper.\n",
    "\n",
    "- Solo se realiza una multiplicación de matrices por Q, K y V, en lugar de repetirla para cada elemento de atención, reduciendo el costo computacional.\n",
    "\n",
    "4. Uso\n",
    "\n",
    "- Se puede usar de manera similar a SelfAttention o CausalAttention en arquitecturas de transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a100e007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape) #la dimension de salida esta controlada por d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b27bb",
   "metadata": {},
   "source": [
    "A modo de comparación:\n",
    "\n",
    "- GPT-2 pequeño: 12 cabezas, vector de contexto de 768.\n",
    "\n",
    "- GPT-2 grande: 25 cabezas, vector de contexto de 1600.\n",
    "\n",
    "En los modelos GPT, los tamaños de incrustación de los tokens y del contexto coinciden (d_in = d_out)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
