{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce9fb0d",
   "metadata": {},
   "source": [
    "# Codificacion de pares de bytes\n",
    "\n",
    "Esta  sección  describe  un  esquema  de  tokenización  más  sofisticado  basado  en  un  concepto  denominado  codificación  de  pares  de  bytes  (BPE).  El  tokenizador  BPE  que  se  describe  en  esta  sección  se  utilizó  para  entrenar  LLM  como  GPT2,  GPT3 y ChatGpt.\n",
    "\n",
    "Para la implentación del algortimo BPE se hace uso de la librería tiktoken, ya que sería demasiado costoso implementarla desde cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c43838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.11.0\n"
     ]
    }
   ],
   "source": [
    "#Comprobación de tiktoken\n",
    "\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f217c",
   "metadata": {},
   "source": [
    "Una vez instalado se puede instanciar el tokenizador BPE desde tiktoken.\n",
    "El uso del tokenizador es similar al tokenizador creado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69a6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "tokenizador = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "#Uso del tokenizador\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "\n",
    "ids = tokenizador.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "\n",
    "#Luego se pueden convertir los ids a texto con el decoder\n",
    "text = tokenizador.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6a149",
   "metadata": {},
   "source": [
    "Se pueden hacer  dos  observaciones  importantes  basándonos  en  los  ID  de  token  y  el  texto  decodificado  anterior.  En  primer  lugar,  al  token  <|endoftext|>  se  le  asigna  un  ID  de  token  relativamente  grande,  concretamente  50256.  De  hecho,  el  tokenizador  BPE,  utilizado  para  entrenar  modelos  como  GPT2,  GPT3  y  el  modelo  original  utilizado  en  ChatGPT,  tiene  un  tamaño  total  de  vocabulario  de  50257,  y  a  <|endoftext|>  se  le  asigna  el  ID  de  token  más  grande.\n",
    "\n",
    "El  algoritmo  subyacente  a  BPE  descompone  las  palabras  que  no  están  en  su  vocabulario  predefinido  en  subpalabras  más  pequeñas  o  incluso  en  caracteres  individuales,  lo  que  le  permite  gestionar  palabras  fuera  de  vocabulario.  Así,  gracias  al  algoritmo  BPE,  si  el  tokenizador  encuentra  una  palabra  desconocida  durante  la  tokenización,  puede  representarla  como  una  secuencia  de  subpalabras  o  caracteres.\n",
    "\n",
    "![Texto alternativo](./imgs/2.9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822898b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizador.encode(\"Akwirw ier\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2ca95",
   "metadata": {},
   "source": [
    "[Agregar tokens de contexto especiales](./5_Muestreo_de_datos_con_una_ventana_deslizante.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
