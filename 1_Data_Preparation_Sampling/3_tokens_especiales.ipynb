{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f599c1",
   "metadata": {},
   "source": [
    "# Agregar tokens de conexto especiales\n",
    "\n",
    "Modificar el  vocabulario  y  el  tokenizador  que  implementamos  en  la  sección  anterior,  SimpleTokenizerV2,  para  admitir  dos  nuevos  tokens,  <|unk|>  y  <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c147e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open(\"../txt/The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    r_text = f.read()\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',r_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b5960",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/2.7.png)\n",
    "\n",
    "Se puede  modificar  el  tokenizador  para  que  use  un  token  <|unk|>  si  encuentra  una  palabra  que  no  forma  parte  del  vocabulario.  Además,  añadimos  un  token  entre  textos  no  relacionados.  Por  ejemplo,  al  entrenar  LLMs  tipo  GPT  con  varios  documentos  o  libros  independientes,  es  habitual  insertar  un  token  antes  de  cada  documento  o  libro  que  sigue  a  una  fuente  de  texto  anterior,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ec0acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"]) #Ampliación del vocabulario con dos tokens más\n",
    "vocab = {w:i for i, w in enumerate(all_tokens)}\n",
    "print(len(vocab))\n",
    "\n",
    "#Comprobaciones\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b4770",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/2.8.png)\n",
    "\n",
    "Al  trabajar  con  múltiples  fuentes  de  texto  independientes,  añadimos  tokens  <|endoftext|>  entre  estos  textos.  Estos  tokens  <|endoftext|>  actúan  como  marcadores,  señalando  el  inicio  o  el  final  de  un  segmento  específico,  lo  que  permite  un  procesamiento  y  una  comprensión  más  eficaces  por  parte  del  LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b2e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "#A parir de la salida anterior se pueden confirmar los dos tokens añadidos\n",
    "#Ajutar el tokeninzador\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):   \n",
    "        self.vocab = vocab #Almacenar el vocabulario para acceder a el en los métodos \n",
    "        self.reverse_vocab = {i:w for w, i in self.vocab.items()} #Vocabulario inverso para el decode, asigna los identificadores a los tokens\n",
    "\n",
    "    #Procesar el texto de entrada en identificadores de tokens\n",
    "    def encode(self, real_text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',real_text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.vocab else  \"<|unk|>\" for item in preprocessed ] #reemplaza  palabras  desconocidas  por  tokens  <|unk|>\n",
    "\n",
    "        ids = [self.vocab[p] for p in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    #Convertir los ids de token en texto\n",
    "    def decode(self, ids):\n",
    "        text = [self.reverse_vocab[id] for id in ids]\n",
    "        text = \" \".join(text)\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #Reemplazar espacios antes de la puntuación especificada \n",
    "        return text\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab=vocab)\n",
    "ids = tokenizer.encode(text)\n",
    "text = tokenizer.decode(ids)\n",
    "\n",
    "print(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae84fb",
   "metadata": {},
   "source": [
    "Hasta  ahora,  se ha analizado  la  tokenización  como  un  paso  esencial  en  el  procesamiento  de  texto  como  entrada  para  los  LLM.  Dependiendo  del  LLM,  algunos  investigadores  también  consideran  tokens  especiales  adicionales,  como  los  siguientes:\n",
    "- [BOS]  (inicio  de  secuencia):  Este  token  marca  el  inicio  de  un  texto.  Indica  al  LLM  dónde  comienza  un  fragmento  de  contenido.\n",
    "- [EOS]  (fin  de  secuencia):  Este  token  se  coloca  al  final  de  un  texto  y  es  especialmente  útil  al  concatenar  varios  textos  no  relacionados,  similar  a  <|endoftext|>.  Por  ejemplo,  al  combinar  dos  artículos  o  libros  diferentes  de  Wikipedia,  el  token  [EOS]  indica  dónde  termina  un  artículo  y  dónde  empieza  el  siguiente.\n",
    "- [PAD]  (relleno):  Al  entrenar  LLM  con  lotes  mayores  a  uno,  estos  pueden  contener  textos  de  longitud  variable.  Para  garantizar  que  todos  los  textos  tengan  la  misma  longitud,  los  textos  más  cortos  se  amplían  o  se  rellenan  con  el  token  [PAD] ,  hasta  la  longitud  del  texto  más  largo  del  lote.\n",
    "\n",
    "[Agregar tokens de contexto especiales](./4_codificacion_bytePair.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
