{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e140c57e",
   "metadata": {},
   "source": [
    "# Incrustaciones de tokens\n",
    "\n",
    "El  último  paso  para  preparar  el  texto  de  entrada  para  el  entrenamiento  LLM  es  convertir  los  identificadores  de  token  en  vectores  de  incrustación\n",
    "\n",
    "![Texto alternativo](./imgs/2.13.png)\n",
    "\n",
    "La  preparación  del  texto  de  entrada  para  un  LLM  implica  tokenizar  el  texto,  convertir  los  tokens  de  texto  en  identificadores  de  token  y  convertir  los  identificadores  de  token  en  vectores  de  incrustación  de  vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bf71f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo de como funciona la coneversion del ID de token en vector de incrustación \n",
    "\n",
    "import torch\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "#Suponer un vocabulario de tamaño 6 y crear inscrustaciones de tamaño 3\n",
    "vocab_tam = 6\n",
    "salida_dim = 3\n",
    "\n",
    "#Instanciar una capa de incrustación en pytorch\n",
    "torch.manual_seed(123)\n",
    "layer = torch.nn.Embedding(vocab_tam, salida_dim)\n",
    "print(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601c4f7",
   "metadata": {},
   "source": [
    "Podemos  ver  que  la  matriz  de  peso  de  la  capa  de  incrustación  contiene  valores  pequeños  y  aleatorios. Estos  valores  se  optimizan  durante  el  entrenamiento  LLM  como  parte  de  la  propia  optimización  LLM. Matriz de ponderación de seis filas y tres columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b8778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Aplicar un ID de token para obtener el vector de incrustación\n",
    "print(layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f66ba9",
   "metadata": {},
   "source": [
    "Si  comparamos  el  vector  de  incrustación  del  ID  de  token  3  con  la  matriz  de  incrustación  anterior,  observamos  que  es  idéntico  a  la  cuarta  fila  (Python  comienza  con  un  índice  cero,  por  lo  que  es  la  fila  correspondiente  al  índice  3).  En  otras  palabras,  la  capa  de  incrustación  es  esencialmente  una  operación  de  búsqueda  que  recupera  filas  de  la  matriz  de  pesos  de  la  capa  de  incrustación  mediante  un  ID  de  token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2502b5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170bc76d",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/2.14.png)\n",
    "\n",
    "Las  capas  de  incrustación  realizan  una  operación  de  búsqueda,  recuperando  el  vector  de  incrustación  correspondiente  al  ID  del  token  de  la  matriz  de  pesos  de  la  capa  de  incrustación.  Por  ejemplo,  el  vector  de  incrustación  del  ID  del  token  5  es  la  sexta  fila  de  la  matriz  de  pesos  de  la  capa  de  incrustación  (es  la  sexta  en  lugar  de  la  quinta  porque  Python  empieza  a  contar  desde  0).  A  modo  de  ilustración,  asumimos  que  los  ID  de  los  tokens  fueron  generados  por  el  vocabulario  pequeño.\n",
    "\n",
    "[Codificación de posiciones de palabras](./7_codificación_posiciones_palabras.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
