{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6664d4",
   "metadata": {},
   "source": [
    "# Muestreo de datos con una ventana deslizante\n",
    "\n",
    "El  siguiente  paso,  antes  de  poder  crear  las  incrustaciones  para  el  LLM,  es  generar  los  pares  de  entradadestino  necesarios  para  entrenar  un  LLM.\n",
    "\n",
    "![Texto alternativo](./imgs/2.10.png)\n",
    "\n",
    "Dada  una  muestra  de  texto,  se  extraen  bloques  de  entrada  como  submuestras  que  sirven  como  entrada  para  el  LLM.  La  tarea  de  predicción  del  LLM  durante  el  entrenamiento  consiste  en  predecir  la  siguiente  palabra  que  sigue  al  bloque  de  entrada.  Durante  el  entrenamiento,  se  enmascaran  todas  las  palabras  que  superan  el  objetivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7a6b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5147\n"
     ]
    }
   ],
   "source": [
    "#Para comrnzar hayr que abrir el documento de verdicts y tokenizarlo\n",
    "import tiktoken\n",
    "\n",
    "with open(\"../txt/The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    r_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "ids_text = tokenizer.encode(r_text)\n",
    "print(len(ids_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de4f1150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287]\n",
      "[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "#Eliminar los 50 priemros tokens del conjunto para hacer un demostración\n",
    "ids_demostracion = ids_text[50:]\n",
    "\n",
    "#  Una  de  las  formas  más  fáciles  e  intuitivas  de  crear  los  pares  de  entradaobjetivo  para  la  tarea  de  predicción  de  la  siguiente  \n",
    "# palabra  es  crear  dos  variables,  x  e  y,  donde  x  contiene  los  tokens  de  entrada  e  y  contiene  los  objetivos,  que  son  las  entradas  \n",
    "# desplazadas  en 1.\n",
    "\n",
    "tam = 4 #num de tokens por entrada\n",
    "x = ids_demostracion[:tam]\n",
    "y = ids_demostracion[1:tam+1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cffc2b",
   "metadata": {},
   "source": [
    "Al  procesar  las  entradas  junto  con  los  objetivos,  que  son  las  entradas  desplazadas  una  posición,  podemos  crear  las  tareas  de  predicción  de  la  siguiente  palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b3ef677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto:  [290] Predicicón:  4920\n",
      "Contexto:  [290, 4920] Predicicón:  2241\n",
      "Contexto:  [290, 4920, 2241] Predicicón:  287\n",
      "Contexto:  [290, 4920, 2241, 287] Predicicón:  257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, tam+1):\n",
    "    context = ids_demostracion[:i]\n",
    "    pred = ids_demostracion[i]\n",
    "    print(\"Contexto: \", context, \"Predicicón: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f12ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto:   and Predicción:   established\n",
      "Contexto:   and established Predicción:   himself\n",
      "Contexto:   and established himself Predicción:   in\n",
      "Contexto:   and established himself in Predicción:   a\n"
     ]
    }
   ],
   "source": [
    "#Ahora con fin ilsutrativos se muestra convirtiendo lostokens en texto\n",
    "for i in range(1, tam+1):\n",
    "    context = ids_demostracion[:i]\n",
    "    pred = ids_demostracion[i]\n",
    "    print(\"Contexto: \", tokenizer.decode(context), \"Predicción: \", tokenizer.decode([pred]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b8c48",
   "metadata": {},
   "source": [
    "Solo  queda  una  tarea  más  antes  de  que  podamos  convertir  los  tokens  en  incrustaciones. Implementar  un  cargador  de  datos  eficiente  que  itere  sobre  el  conjunto  de  datos  de  entrada  y  devuelva  las  entradas  y  los  destinos  como  tensores  de  PyTorch,  que  pueden  considerarse  como  matrices  multidimensionales.\n",
    "\n",
    "![Texto alternativo](./imgs/2.11.png)\n",
    "\n",
    "Para  implementar  cargadores  de  datos  eficientes,  recopilamos  las  entradas  en  un  tensor,  x,  donde  cada  fila  representa  un  contexto  de  entrada.  Un  segundo  tensor,  y,  contiene  los  objetivos  de  predicción  correspondientes  (palabras siguientes),  que  se  crean  desplazando  la  entrada  una  posición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2318f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_lenght, stride):\n",
    "        self.input_ids = []\n",
    "        self.labels_ids = []\n",
    "\n",
    "        tokens_ids = tokenizer.encode(txt) #Tokenzar el texto completo\n",
    "\n",
    "        for i in range(0, len(tokens_ids) - max_lenght, stride): #Uso de una  ventana  deslizante  para  dividir  el  libro  en  secuencias  superpuestas  de  longitud  máxima\n",
    "            self.input_ids.append(torch.tensor(tokens_ids[i:i+max_lenght]))\n",
    "            self.labels_ids.append(torch.tensor(tokens_ids[i+1:i+max_lenght+1]))\n",
    "\n",
    "    def __len__(self): #Devuelve  el  número  total  de  filas  en  el  conjunto  de  datos\n",
    "        return len(self.labels_ids)\n",
    "    \n",
    "    def __getitem__(self, index): #Devuelve  una  sola  fila  del  conjunto  de  datos\n",
    "        return self.input_ids[index], self.labels_ids[index] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2476738",
   "metadata": {},
   "source": [
    "La  clase  GPTDatasetV1  del  listado se  basa  en  la  clase  Dataset  de  PyTorch  y  define  cómo  se  obtienen  las  filas  individuales  del  conjunto  de  datos.  Cada  fila  consta  de  un  número  de  identificadores  de  token  (basados  en  una  longitud  máxima)  asignados  a  un  tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c403b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
      "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
      "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n",
      "[tensor([[ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
      "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]]), tensor([[ 3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016]])]\n"
     ]
    }
   ],
   "source": [
    "#Creacion de los dataloaders(cargadores de datos)\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "       stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "   tokenizer = tiktoken.get_encoding(\"gpt2\")         #inicializar toenizer          \n",
    "   dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)    #Crear dataset\n",
    "   dataloader = DataLoader( \n",
    "       dataset,\n",
    "       batch_size=batch_size,\n",
    "       shuffle=shuffle,\n",
    "       drop_last=drop_last,         #descarta  el  último  lote  si  es  más  corto  que  el  tamaño  de  lote  especificado                          \n",
    "       num_workers=num_workers               #La  cantidad  de  procesos  de  CPU                         \n",
    "   )\n",
    "   return dataloader\n",
    "\n",
    "#probar el cargador de daros con un tamaño de lote de 1 para una LLM con un tamaño de contexto de 4.\n",
    "\n",
    "dl = create_dataloader_v1(r_text, batch_size=2, max_length=8, stride=2, shuffle=False)\n",
    "data_iter = iter(dl)\n",
    "fr_batch = next(data_iter)\n",
    "print(fr_batch)\n",
    "sc_batch = next(data_iter)\n",
    "print(sc_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc325a88",
   "metadata": {},
   "source": [
    "Si  comparamos  el  primer  lote  con  el  segundo,  observamos  que  los  ID  de  token  del  segundo  lote  se  desplazan  una  posición  con  respecto  al  primero  (por  ejemplo,  el  segundo  ID  en  la  entrada  del  primer  lote  es  367,  que  es  el  primer  ID  de  la  entrada  del  segundo  lote).  La  configuración  de  paso  determina  el  número  de  posiciones  que  se  desplazan  las  entradas  entre  lotes,  emulando  un  enfoque  de  ventana  deslizante\n",
    "\n",
    "![Texto alternativo](./imgs/2.12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f516f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 2119,    13,   198,   198],\n",
      "        [   12, 12239,    13,   198],\n",
      "        [  402,   271, 10899,   338],\n",
      "        [  319,   257,  1308, 27461],\n",
      "        [  257,  8212,   326, 13663],\n",
      "        [ 1813,   340,   284,   502],\n",
      "        [   11,   355,  9074,    13],\n",
      "        [12375,   299, 20896,    82]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   13,   198,   198,     1],\n",
      "        [12239,    13,   198,   198],\n",
      "        [  271, 10899,   338, 12036],\n",
      "        [  257,  1308, 27461,  1171],\n",
      "        [ 8212,   326, 13663,   262],\n",
      "        [  340,   284,   502,    11],\n",
      "        [  355,  9074,    13,   536],\n",
      "        [  299, 20896,    82, 24357]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(r_text, batch_size=8, max_length=4, stride=4)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17727ceb",
   "metadata": {},
   "source": [
    "Tenga  en  cuenta  que  aumentamos  el  paso  a  4.  Esto  es  para  utilizar  el  conjunto  de  datos  en  su  totalidad  (no  omitimos  ni  una  sola  palabra)  pero  también  para  evitar  cualquier  superposición  entre  los  lotes,  ya  que  una  mayor  superposición  podría  llevar  a  un  mayor  sobreajuste.\n",
    "\n",
    "[Agregar tokens de contexto especiales](./6_incrustaciones_tokens.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
