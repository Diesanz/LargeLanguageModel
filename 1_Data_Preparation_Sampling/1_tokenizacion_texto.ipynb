{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c28fd33",
   "metadata": {},
   "source": [
    "# Tokenización de Texto\n",
    "Esta  sección  explica  cómo  dividimos  el  texto  de  entrada  en  tokens  individuales,  un  paso  de  preprocesamiento  necesario  para  crear  incrustaciones  para  un  LLM.  Estos  tokens  son  palabras  individuales  o  caracteres  especiales,  incluyendo  signos  de  puntuación.\n",
    "\n",
    "![Texto alternativo](/imgs/2.3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fbd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de carcateres: 20483\n",
      "Primeras 99 palabras: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "#Tokenizer texto de  Edith  Wharton  titulado\n",
    "with open(\"../txt/The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    r_text = f.read()\n",
    "print(f'Total de carcateres: {len(r_text)}')\n",
    "print(f'Primeras 99 palabras: {r_text[:99]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a96079",
   "metadata": {},
   "source": [
    "Nuestro  objetivo  es  convertir  este  cuento  corto  de  20.479  caracteres  en  palabras  individuales  y  caracteres  especiales  que  luego  podamos  convertir  en  incorporaciones  para  la  capacitación  LLM .\n",
    "Para  ello,  haremos  una  pequeña  incursión  y  usaremos  la  biblioteca  de  expresiones  regulares  de  Python,  re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1270470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ' ', 'mundo.', ' ', 'Esto', ' ', 'es', ' ', 'una', ' ', 'prueba.']\n",
      "['Hola', ' ', 'mundo', '.', '', ' ', 'Esto', ' ', 'es', ' ', 'una', ' ', 'prueba', '.', '']\n",
      "Ahora sin espacio:  ['Hola', 'mundo', '.', 'Esto', 'es', 'una', 'prueba', '.']\n",
      "Ahora admitiendo otros signos de puntuacion:  ['Hola', ',', 'mundo', '.', 'Esto', 'es', '--', 'una', 'prueba', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hola mundo. Esto es una prueba.\"\n",
    "res = re.split(r'(\\s)',text) #Lista de palabras individuales, espacios en blanco\n",
    "print(res)\n",
    "\n",
    "#Ahora tner entradas de signos de puntuación independientes a sus palabras\n",
    "res = re.split(r'([,.]|\\s)', text)\n",
    "print(res)\n",
    "\n",
    "#Eliminar espacios en blanco\n",
    "res = [t for t in res if t.strip()] #Simplificar y abreviar los resultados\n",
    "print(\"Ahora sin espacio: \", res)\n",
    "\n",
    "#Tener en cuenta otros signos de puntuacion\n",
    "text = \"Hola, mundo. Esto es-- una prueba?\"\n",
    "res = re.split(r'([,.:?_!\"()\\']|--|\\s)', text)\n",
    "res = [t for t in res if t.strip()]\n",
    "print(\"Ahora admitiendo otros signos de puntuacion: \", res)\n",
    "#Division  del  texto  en  palabras  y  signos  de  \n",
    "#puntuación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914512e3",
   "metadata": {},
   "source": [
    "![Texto alternativo](../imgs/2.4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70175525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "Primeros 30 tokens: ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "#Aplicar el tokenizador básico al .txt\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',r_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(\"Primeros 30 tokens:\", preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28d9d6",
   "metadata": {},
   "source": [
    "[Conversión  de  tokens  en  ID  de  tokens](./2_tokens_to_ids.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
