{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ead0fe",
   "metadata": {},
   "source": [
    "#  Conversión  de  tokens  en  ID  de  tokens\n",
    "En  la  sección  anterior,  convertimos  un  cuento  de  Edith  Wharton  en  tokens  individuales.\n",
    "En  esta  sección,  convertiremos  estos  tokens  de  una  cadena  de  Python  a  una  representación  entera  para  generar  los  llamados  ID  de  token.  Esta  conversión  es  un  paso  intermedio  antes  de  convertir  los  ID  de  token  en  vectores  de  incrustación.\n",
    "\n",
    "Para  asignar  los  tokens  generados  previamente  a  sus  identificadores,  primero  debemos  crear  un  vocabulario.  Este  vocabulario  define  cómo  asignamos  cada  palabra  única  y  carácter  especial  a  un  entero  único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad79b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lectura y tokenizacion\n",
    "import re\n",
    "with open(\"../txt/The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    r_text = f.read()\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',r_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebd9d2",
   "metadata": {},
   "source": [
    "![Texto alternativo](../imgs/2.5.png)\n",
    "\n",
    "Creamos  un  vocabulario  tokenizando  todo  el  texto  de  un  conjunto  de  datos  de  entrenamiento  en  tokens  individuales.  Estos  tokens  individuales  se  ordenan  alfabéticamente  y  se  eliminan  los  duplicados.  Los  tokens  únicos  se  agregan  en  un  vocabulario  que  define  una  asignación  de  cada  token  único  a  un  valor  entero  único.  El  vocabulario  mostrado  es  deliberadamente  pequeño  para  fines  ilustrativos  y  no  contiene  puntuación  ni  caracteres  especiales  para  simplificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95013da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "#Convertir  estos  tokens  de  una  cadena  de  Python  a  una  representación  entera  para  \n",
    "#generar  los  llamados  ID  de  token.\n",
    "\n",
    "#Primero hay que crear un vocabulario para cada palabra única\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_tam = len(all_words)\n",
    "print(vocab_tam)\n",
    "\n",
    "vocab = {w:i for i,w in enumerate(all_words)}\n",
    "#Imprimir 50 primeros\n",
    "for i, w in enumerate(vocab.items()):\n",
    "    print(w)\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257cf17",
   "metadata": {},
   "source": [
    "Como  podemos  ver,  según  el  resultado  anterior,  el  diccionario  contiene  tokens  individuales  asociados  con  etiquetas  enteras  únicas.  Nuestro  siguiente  objetivo  es aplicar  este  vocabulario  para  convertir  el  nuevo  texto  en  identificadores  de  token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65094707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "#Implementacón  una  clase  tokenizadora  completa  en  Python  con  un  método  encode  que  divide  el  texto  en  tokens  y  realiza  la  \n",
    "#conversión  de  cadena  a  entero  para  generar  identificadores  de  token  mediante  el  vocabulario.  Además,  implementamos  un  método  \n",
    "#decode  que  realiza  la  conversión  inversa  de  entero  a  cadena  para  convertir  los  identificadores  de  token  de  nuevo  en  tex\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):   \n",
    "        self.vocab = vocab #Almacenar el vocabulario para acceder a el en los métodos \n",
    "        self.reverse_vocab = {i:w for w, i in self.vocab.items()} #Vocabulario inverso para el decode, asigna los identificadores a los tokens\n",
    "    #Procesar el texto de entrada en identificadores de tokens\n",
    "    def encode(self, real_text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',real_text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        ids = [self.vocab[p] for p in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    #Convertir los ids de token en texto\n",
    "    def decode(self, ids):\n",
    "        text = [self.reverse_vocab[id] for id in ids]\n",
    "        text = \" \".join(text)\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #Reemplazar espacios antes de la puntuación especificada \n",
    "        return text\n",
    "\n",
    "    \n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text =  \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable \n",
    "pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "text = tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e30645",
   "metadata": {},
   "source": [
    "![Texto alternativo](../imgs/2.6.png)\n",
    "\n",
    "\n",
    "[Agregar tokens de contexto especiales](./3_tokens_especiales.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
