{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e11d7bb",
   "metadata": {},
   "source": [
    "# Normalización de activaciones con normalización de capas\n",
    "\n",
    "Entrenar  redes  neuronales  profundas  con  múltiples  capas  puede resultar  a  veces  complicado  debido  a  problemas  como  la  desaparición  o  la  explosión  de  gradientes.  Estos  problemas  generan  dinámicas  de  entrenamiento  inestables  y  dificultan  que  la  red  ajuste  eficazmente  sus  pesos,  lo  que  implica  que  el  proceso  de  aprendizaje  tiene dificultades  para  encontrar  un  conjunto  de  parámetros  (pesos)  que  minimice  la  función  de  pérdida.  En  otras  palabras,  la  red  tiene  dificultades  para  aprender  los  patrones  subyacentes  en  los  datos  a  un  nivel  que  le  permita  realizar  predicciones  o  tomar  decisiones  precisas.\n",
    "\n",
    "En estas sección se implementará   la  normalización  de  capas  para  mejorar  la  estabilidad  y Eficiencia  del  entrenamiento  de  redes  neuronales.\n",
    "\n",
    "La  idea  principal  detrás  de  la  normalización  de  capas  es  ajustar  las  activaciones  (salidas)  de  una  capa  de  red  neuronal  para  tener  una  media  de  0  y  una  varianza  de  1,  también  conocida  como  varianza  unitaria.\n",
    "Este  ajuste  acelera  la  convergencia  a  pesos  efectivos  y  garantiza  un  entrenamiento  consistente  y  fiable.  \n",
    "\n",
    "Descripción general visual de cómo funciona la normalización de capas.\n",
    "\n",
    "![Texto alternativo](./imgs/4.5.png)\n",
    "\n",
    "Se puede recrear el ejemplo recreado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2e452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #dos ejemplos de cinco características\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "output = layer(batch_example)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0474af2",
   "metadata": {},
   "source": [
    "La  capa  de  red  neuronal  que  hemos  codificado  consta  de  una  capa  lineal  seguida  de  una  función  de  activación  no  lineal,  ReLU  (abreviatura  de  Unidad  Lineal  Rectificada),  que  es  una  función  de  activación  estándar  en  redes  neuronales.  \n",
    "\n",
    "Antes de aplicar la normalización de capas a las salidas, hay que examinar la media y la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84717ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media:  tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Var:  tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = output.mean(dim=-1, keepdim=True) #alcula la media a lo largo de la última dimensión del tensor output\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "print(\"Media: \", mean)\n",
    "print(\"Var: \", var)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f6b36",
   "metadata": {},
   "source": [
    "La  primera  fila  del  tensor  de  media  anterior  contiene  el  valor  medio  de  la  primera  fila  de  entrada,  y  la  \n",
    "segunda  fila  de  salida  contiene  la  media  de  la  segunda  fila  de  entrada.\n",
    "El  uso  de  keepdim=True  en  operaciones  como  el  cálculo  de  la  media  o  la  varianza  garantiza  que  el  tensor  de  salida  conserve  el  mismo  número  de  dimensiones  que  el  tensor  de  entrada,  incluso  si  la  \n",
    "operación  reduce  el  tensor  según  la  dimensión  especificada  mediante  dim.  \n",
    "Por  ejemplo,  sin  keepdim=True,  el  tensor  de  la  media  devuelto  sería  un  vector  bidimensional  [0,1324,0,2170]  en  lugar  de  una  matriz  de  2×1  dimensiones  [[0,1324],  [0,2170]]. El  parámetro  dim  especifica  la  dimensión  a  lo  largo  de  la  cual  se  realiza  el  cálculo  de  la  estadística (aquí,  media  o  varianza)  debe  realizarse  en  forma  de  tensor.\n",
    "\n",
    "![Texto alternativo](./imgs/4.6.png)\n",
    "\n",
    "Al  añadir  la  normalización  de  capas  al  modelo  GPT,  que  genera  tensores  3D  con  forma  [batch_size,  num_tokens,  embedding_size],  podemos  seguir  usando  dim=1  para  la  normalización  en  la  última  dimensión,  evitando  así  un  cambio  de  dim=1  a  dim=2.\n",
    "\n",
    "A  continuación,  se aplicará la  normalización  de  capas  a  las  salidas  de  capa  obtenidas  anteriormente.  \n",
    "La  operación  consiste  en  restar  la  media  y  dividirla  entre  la  raíz  cuadrada  de  la  varianza  (también  \n",
    "conocida  como  desviación  estándar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f8b7025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (output - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dc4dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Para mejorar la visualizació  se puede elimianr la notación cientiífica\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14461e",
   "metadata": {},
   "source": [
    "Hasta  ahora,  en  esta  sección,  hemos  codificado  y  aplicado  la normalización  de  capas  paso  a  paso.  Ahora,  encapsulemos  este  proceso  en  un  módulo  de  PyTorch  que  podremos  usar  posteriormente  en  el  modelo  GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3ceb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cuando losdatos pasas por muchas capas, los valores de activación pueden volverse muy grandes o muy pequeños y dificultar el entrenamiento.\n",
    "#La normalización de capa soluciona esto estabilizando las activaciones por cada muestra y token individual, no por batch como BatchNorm.\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6fb54e",
   "metadata": {},
   "source": [
    "Esta  implementación  específica  de  la  normalización  de  capas  opera  en  la  última  dimensión  del  tensor  de  entrada  x,  que  representa  la  dimensión  de  incrustación  (emb_dim).  La  variable  eps  es  una  pequeña  constante  (épsilon)  que  se  añade  a  la  varianza  para  evitar  la  división  por  cero  durante  la  normalización.  \n",
    "\n",
    "La  escala  y  el  desplazamiento  son  dos  parámetros  entrenables  (de  la  misma  dimensión  que  la  entrada)  que  el  LLM  ajusta  automáticamente  durante  el  entrenamiento  si  se  determina  que  esto  mejoraría  el  rendimiento  del  modelo  en  su  tarea  de  entrenamiento.  Esto  permite  que  el  modelo  aprenda  el  escalado  y  el  desplazamiento  adecuados  para  los  datos  que  procesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "892c720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "print(out_ln)\n",
    "#Comprobación de que layerNorm funciona bien\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58deb1",
   "metadata": {},
   "source": [
    "Hasta ahora se han cubierto los comoponentes básicos para implementar la aquitectura GPT.\n",
    "\n",
    "![Texto alternativo](./imgs/4.7.png)\n",
    "\n",
    "En la siguiente sección se implementará la función de activación GLU.\n",
    "\n",
    "[Implementación de una red de propagación hacia delante con activaciones GELU](./3_propagación_adelante_GELU.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
