{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7ee1be",
   "metadata": {},
   "source": [
    "# Evaluación de modelos de texto generativo\n",
    "\n",
    "Se comenzará la sección configurando el LLM para la generación de texto a partir del código implementado en la sección 3.\n",
    "\n",
    "![Texto alternativo](./imgs/5.2.png)\n",
    "\n",
    "La  siguiente  subsección  resume  la  generación  de  texto  que se configuró al final de la sección 3, antes  de  sumergirnos  en  la  evaluación  del  texto  y  el  cálculo  \n",
    "de  las  pérdidas  de  entrenamiento  y  validación  en  las  subsecciones  posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37007970",
   "metadata": {},
   "source": [
    "### 1.1.1 Uso de GPT para generar texto\n",
    "\n",
    "En esta subsección se configura el LLm y se recapitula brevemente el proceso de generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d306ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtiene la ruta de la carpeta principal del proyecto (subiendo un nivel desde seccion05)\n",
    "ruta_proyecto_principal = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Añade esta ruta a la lista de lugares donde Python busca módulos\n",
    "if ruta_proyecto_principal not in sys.path:\n",
    "    sys.path.append(ruta_proyecto_principal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c794100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 256, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from seccion04_ImplementacionGPTGeneracionTexto.gptModel import GPTModel\n",
    "from seccion04_ImplementacionGPTGeneracionTexto.gptConfig124M import GPT_CONFIG_124M\n",
    "\n",
    "#importacion de librerias necesarias y acortamiento del context length de la configuracion de GPT (1024 tokens a  256)\n",
    "GPT_CONFIG_124M[\"context_length\"] = 256\n",
    "print(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb16f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b2676",
   "metadata": {},
   "source": [
    "Esta  modificación  reduce  las  demandas  computacionales  del  entrenamiento  del  modelo,  haciendo  posible  realizar  el  entrenamiento  en  una  computadora  portátil  estándar.\n",
    "\n",
    "Originalmente,  el  modelo  GPT2,  con  124  millones  de  parámetros,  se  configuró  para  gestionar  hasta  1024  tokens. \n",
    "\n",
    "Utilizando  la  instancia  GPTmodel ,  adoptamos  la  función  generate_text_simple  presentada anteriorment e se introducen dos  funciones  útiles:  text_to_token_ids  y  token_ids_to_text.  Estas  funciones  facilitan  la  conversión  entre  representaciones  de  texto  y  tokens.\n",
    "\n",
    "![Texto alternativo](./imgs/5.3.png)\n",
    "\n",
    "- Primero,  el  tokenizador  convierte  el  texto  de  entrada  en  una  serie  de  identificadores  de  token.\n",
    "- Segundo,  el  modelo  recibe  estos  identificadores  de  token  y  genera  los  logits  correspondientes,  que  son  vectores  que  representan  la  distribución  de  probabilidad  de  cada  token  del  vocabulario.\n",
    "-  Tercero,  estos  logits  se  convierten  de  nuevo  en  identificadores  de  token,  que  el  tokenizador  decodifica  en  texto  legible,  completando  así  el  ciclo  de  entrada  a  salida  textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38383d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "from seccion04_ImplementacionGPTGeneracionTexto.generateTextSimple import generate_text_simple\n",
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    # 1. squeeze(0): Quita la dimensión del \"lote\" que añadió unsqueeze.\n",
    "    #Es como sacar la lista de la caja para poder leerla.\n",
    "    #Ej: tensor([[25134]]) -> tensor([25134]), forma: [1]\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    # 2. Convierte el tensor 1D a una lista y luego a texto.\n",
    "    # Ej: tensor([25134]) -> [25134] -> \"hola\"\n",
    "\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525554c",
   "metadata": {},
   "source": [
    "El modelo sigue sin producir texto coherente porque todavia no se ha entrenado.\n",
    "\n",
    "Para  definir  qué  hace  que  un  texto  sea  \"coherente\"  o  de  \"alta  calidad\", se debe implementar un  método  numérico  para  evaluar  el  contenido  generado.  Este  enfoque  nos  permitirá  monitorear  y  mejorar  el  rendimiento  del  modelo  durante  su  proceso  de  entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a5af7",
   "metadata": {},
   "source": [
    "### 1.1.2 Cálculo de la perdida de generación de texto\n",
    "\n",
    "En esta subsección se exploraran técnicas para evaluar numericamnte la calidad del texto generado durante el entrenamiento mediante el cálculo de la llamada función de pérdidda de generación de texto.\n",
    "\n",
    "![Texto alternativo](./imgs/5.1.png)\n",
    "\n",
    "Es necesario realizar todos estos pasos indicados en la figura antes de calcular una pérdida que mida la calidad del texto generado.\n",
    "\n",
    "La figura describe  el  proceso  de  generación  de  texto  con  un  vocabulario  reducido  de  7  tokens  para  que  esta  imagen  quepa  en  una  sola  página.  Sin  embargo,  GPTModel  trabaja  con  un  vocabulario  mucho  mayor,  compuesto  por  50 257  palabras;  por  lo  tanto,  los  ID  de  token  en  los  siguientes  códigos  estarán  comprendidos  entre  0  y  50 256,  en  lugar  de  entre  0  y  6.\n",
    "\n",
    "Definición de dos entradas con sus objetivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a662b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 1\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                        [40, 1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeee0d9",
   "metadata": {},
   "source": [
    "Los  objetivos  son  las  entradas,  pero  desplazados  una  posición  hacia  adelante.\n",
    "Esta  estrategia  de  desplazamiento  es  crucial  para  enseñar  al  modelo  a  predecir  el  siguiente  token  en  una  secuencia. Cuando  introducimos  las  entradas  en  el  modelo  para  calcular  vectores  logit  para  los  dos  ejemplos  de  entrada,  cada  uno  compuesto  por  tres  tokens,  y  aplicamos  la  función  softmax  para  transformar  estos  valores  logit  en  puntuaciones  de  probabilidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bbe6e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "Ids token:  tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#Paso 2: logits a scores de probabilidad\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape) \n",
    "\n",
    "#Paso 3 y 4: argmax para obtener el correspondiente token ID \n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Ids token: \", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84067cbb",
   "metadata": {},
   "source": [
    "Dado  que  se tiene  2  lotes  de  entrada,  cada  uno  con  3  tokens,  aplicar  la  función  argmax  a  los  puntajes  de  probabilidad  produce  2  conjuntos  de  salidas,  cada  uno  con  3  identificaciones  de  tokens  predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52b73d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "#Paso 5: Convertir IDs a texto\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\") #verdadera salida\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\") #salida del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f97d7",
   "metadata": {},
   "source": [
    "El  modelo  produce  texto  aleatorio  diferente  del  texto  objetivo  porque  aún  no  se  ha  entrenado. \n",
    "\n",
    "Ahora se debe evaluar numéricamente el rendimiento del texto generado por el modelo mediante la llamada pérdida. Esto no es solo útil para medir la calidad del texto generado, sino que también es un elemento fundamental para implementar posteriormente la función de entrenamiento, que se utiliza para actualizar el peso del modelo y mejorar el texto generado.\n",
    "\n",
    "![Texto alternativo](./imgs/5.5.png)\n",
    "\n",
    "Parte  del  proceso  de  evaluación  de  texto consiste  en  medir  la  distancia  entre  los  tokens  generados  y  las  predicciones  correctas  (objetivos). La función de entrenamiento que se implementará utilizará esta información para ajustar los pesos del modelo y generar un texto más similar.\n",
    "El  entrenamiento  del  modelo  busca  aumentar  la  probabilidad  softmax  en  las  posiciones  de  índice  correspondientes  a  los  ID  de  token  objetivo  correctos.\n",
    "\n",
    "![Texto alternativo](./imgs/5.6.png)\n",
    "\n",
    "Antes  del  entrenamiento,  el  modelo  genera  vectores  aleatorios  de  probabilidad  del  siguiente  token.  El  objetivo  del  entrenamiento  del  modelo  es  garantizar  que  los  valores  de  probabilidad  correspondientes  a  los  ID  de  token  objetivo  resaltados  sean maximizado.\n",
    "\n",
    "Para  cada  uno  de  los  dos  textos  de  entrada,  podemos  imprimir  las  puntuaciones  de  probabilidad  softmax  iniciales correspondiente  a  los  tokens  de  destino  a  través  del  siguiente  código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764aaa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9108e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]] #porbabilidades asociadas a los objetivos (cuales inidice son los que se quieren que salgan)\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664231",
   "metadata": {},
   "source": [
    "El  objetivo  de  entrenar  un  LLM  es  maximizar  estos  valores,  intentando  acercarlos  lo  más  posible  a  una  probabilidad  de  1.  De  esta  manera,  garantizamos  que  el  LLM  elija  consistentemente  el  token  objetivo  (esencialmente,  la  siguiente  palabra  de  la  oración)  como  el  siguiente  token  que  genera.\n",
    "\n",
    "***Retropropagación***\n",
    "¿Cómo  maximizamos  los  valores  de  probabilidad  softmax  correspondientes  a  los  tokens  objetivo?  \n",
    "En  resumen,  actualizamos  los  pesos  del  modelo  para  que  este  genere  valores  más  altos  para  los  respectivos  ID  de  token  que  queremos  generar.  La  actualización  de  los  pesos  se  realiza  mediante  un  proceso  llamado  retropropagación,  una  técnica  estándar  para  el  entrenamiento  de  redes  neuronales  profundas.\n",
    "\n",
    "La  retropropagación  requiere  una  función  de  pérdida  que  calcula  la  diferencia  entre  el  resultado  previsto  del  modelo  (en  este  caso,  las  probabilidades  correspondientes  a  los  ID  de  token  objetivo)  y  el  resultado  deseado  real.  Esta  función  de  pérdida  mide  la  distancia  entre  las  predicciones  del  modelo  y  los  valores  objetivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c753d6",
   "metadata": {},
   "source": [
    "***Perdida de entropia cruzada***\n",
    "\n",
    "![Texto alternativo](./imgs/5.7.png)\n",
    "\n",
    "Los  pasos  1  a  3  calculan  las  probabilidades  de  los  tokens  correspondientes  a  los  tensores  objetivo.  Estas  probabilidades  se  transforman  mediante  un  logaritmo  y  se  promedian  en  los  pasos  4  a  6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac4aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -10.1492,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc51f25",
   "metadata": {},
   "source": [
    "Trabajar  con  logaritmos  de  puntuaciones  de  probabilidad  es  más  manejable  en  la  optimización  matemática  que  manejar  las  puntuaciones  directamente. \n",
    "A  continuación,  combinamos  estas  probabilidades  logarítmicas  en  una  única  puntuación  calculando  el  promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf5692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.5722)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fa00f",
   "metadata": {},
   "source": [
    "El  objetivo  es  obtener  la  probabilidad  logarítmica  promedio  lo  más  cerca  posible  de  0  actualizando  los  pesos  del  modelo  como  parte  del  proceso  de  entrenamiento.\n",
    "\n",
    "Sin  embargo,  en  el  aprendizaje  profundo,  la  práctica  común  no  es  aumentar  la  probabilidad  logarítmica  promedio  hasta  0,  sino  reducir  la  probabilidad  logarítmica  promedio  negativa  a  0.  La  probabilidad  logarítmica  promedio  negativa  es  simplemente  la  probabilidad  logarítmica  promedio  multiplicada  por  1,  lo  que  corresponde  al  paso  6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa127aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5722)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd172d0",
   "metadata": {},
   "source": [
    "El  término  para  este  valor  negativo,  10,7940  convirtiéndose  en  10,7940,  se  conoce  como  la  cruz depérdida  de  entropía  en  el  aprendizaje  profundo.\n",
    "\n",
    "***PyTorch  resulta  útil  en  este  caso,  ya  que  tiene  una  función  cross_entropy  incorporada  que  se  encarga  de  todos  estos  6  pasos***\n",
    "\n",
    "### Pérdida de entropia cruzada\n",
    "En  esencia,  la  pérdida  de  entropía  cruzada  es  una  medida  popular  en  el  aprendizaje  automático  y  el  aprendizaje  profundo  que  mide  la  diferencia  entre  dos  distribuciones  de  probabilidad:  típicamente,  la  distribución  real  de  etiquetas  (aquí,  tokens  en  un  conjunto  de  datos)  y  la  distribución  prevista  a  partir  de  un  modelo  (por  ejemplo,  las  probabilidades  de  token  generadas  por  un  LLM).\n",
    "En  el  contexto  del  aprendizaje  automático  y  específicamente  en  marcos  como  PyTorch,  la  función  cross_entropy  calcula  esta  medida  para  resultados  discretos,  que  es  similar  a  la  probabilidad  logarítmica  promedio  negativa  de  los  tokens  de  destino  dadas  las  probabilidades  de  token  generadas  por  el  modelo,  lo  que  hace  que  los  términos  entropía  cruzada  y  probabilidad  logarítmica  promedio  negativa  estén  relacionados  y  a  menudo  se  usen  indistintamente  en  la  práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084386da",
   "metadata": {},
   "source": [
    "Anteriormente,  se aplicó la  función  softmax,  seleccionamos  los  puntajes  de  probabilidad  correspondientes  a  los  identificadores  de  destino  y  calculamos  las  probabilidades  logarítmicas  promedio  negativas.\n",
    "La  función  cross_entropy  de  PyTorch  se  encargará  de  todos  estos  pasos  por  nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eab8634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "tensor(10.5722)\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea338ef8",
   "metadata": {},
   "source": [
    "La pérdida resultante es la misam que se obtuvo al aplicar manualmente los pasos individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce9f9c",
   "metadata": {},
   "source": [
    "### 1.1.3 Cálculo de las pérdidas del conjunto de entrenamieto y validación\n",
    "\n",
    "En esta subsección , primero se prepararan los cconjuntos de datos de entrenamiento y validación que se utilizará para entrenar el LLM.\n",
    "Seguidamente se calculará la entropía cruzada para los conjuntos.\n",
    "\n",
    "![Texto alternativo](./imgs/5.8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bde9ab",
   "metadata": {},
   "source": [
    "Para poder calcular la pérdida en los conjuntos de entrenamiento y validación, se va a utilizar un conjunto de datos pequeño del cuento **The Verdict**.\n",
    "\n",
    "### EL costo de la formación previa de LLMS\n",
    "Para  poner  en  perspectiva  la  escala  de  nuestro  proyecto,  consideremos  el  entrenamiento  del  modelo  Llama  2  de  7  mil  millones  de  parámetros,  un  modelo  LLM  relativamente  popular  y  de  libre  acceso.  Este  modelo  requirió  184 320  horas  de  GPU  en  costosas  GPU  A100,  procesando  2  billones  de  tokens.  Al  momento  de  escribir  este  artículo,  ejecutar  un  servidor  en  la  nube  8xA100  en  AWS  cuesta  alrededor  de  30$   por  hora.  Una  estimación  aproximada  sitúa  el  costo  total  de  entrenamiento  de  dicho  modelo  LLM  en  unos  690 000$  (calculado  como  184 320  horas  divididas  entre  8  y  multiplicadas  por  30$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b11c8f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20483\n",
      "Tokens: 5147\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../txt/The_Verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "#Despues de cargar el dataset, se puede chequear el número de caracteres y tokens\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22bd3e",
   "metadata": {},
   "source": [
    "Con  tan  solo  5145  tokens,  el  texto  podría  parecer  demasiado  pequeño  para  entrenar  un  LLM,  pero  como  se  mencionó  anteriormente,  su  propósito  es  educativo  para  que  podamos  ejecutar  el  código  en  minutos  en  lugar  de  semanas. \n",
    "\n",
    "A  continuación,  se dividen  el  conjunto  de  datos  en  un  conjunto  de  entrenamiento  y  uno  de  validación,  y utilizando los  cargadores  de  datos  de la sección 2  para  preparar  los  lotes  para  el  entrenamiento  LLM. \n",
    "\n",
    "![Texto alternativo](./imgs/5.9.png)\n",
    "\n",
    "En los cargadores de datos que se van a crear se va a establecer max_length igual a la longitud de contexto de 256 tokens que admite el LLM para que el LLm vea textos más largos durante el entrenemiento.\n",
    "\n",
    "Para  implementar  la  división  y  carga  de  datos  visualizada  en  la  Figura,  primero  definimos  un  train_ratio  para  utilizar  el  90  %  de  los  datos  para  entrenamiento  y  el  10  %  restante  como  datos  de  validación  para  la  evaluación  del  modelo  durante  el  entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcdd6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[: split_idx]\n",
    "val_data = text_data[split_idx :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa501d9",
   "metadata": {},
   "source": [
    "Usando train data y val data, se pueden crear ahora los respectivos data loader reutilizando las funciones de la sección 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76b395d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seccion02_TrabajarDatosTexto.dataloader_v1 import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be4eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eb460",
   "metadata": {},
   "source": [
    "Según  el  código  de  salida  anterior,  tenemos  9  lotes  de  conjuntos  de  entrenamiento  con  2  muestras  y  256  tokens  cada  uno.  Dado  que  solo  asignamos  el  10  %  de  los  datos  a  la  validación,  solo  hay  un  lote  de  validación  con  2  ejemplos  de  entrada.\n",
    "\n",
    "Como  se  esperaba,  los  datos  de  entrada  (x)  y  los  datos  de  destino  (y)  tienen  la  misma  forma  (el  tamaño  del  lote  multiplicado  por  la  cantidad  de  tokens  en  cada  lote)  ya  que  los  destinos  son  las  entradas  desplazadas  una  posición.\n",
    "\n",
    "Implementación de una  función  de  utilidad  para  calcular  la  pérdida  de  entropía  cruzada  de  un  lote  determinado  devuelto  a  través  del  cargador  de  entrenamiento  y  validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61baa0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, taget_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), taget_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1654d",
   "metadata": {},
   "source": [
    "Ahora se puede usar esta  función  de  utilidad  calc_loss_batch ,  que  calcula  la  pérdida  de  un  solo  lote,  para  implementar  la  siguiente  función  calc_loss_loader  que  calcula  la  pérdida  de  todos  los  lotes  muestreados  por  un  cargador  de  datos  determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)                 #Iterativo sobre el numero de lotes si no se especifica un numero fijo de lotes           \n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))          #reducir el numero de lotes para que coicida con el numero total de lotes en el cargador de datos\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):   \n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()                           #suma de perdidas por cada lote\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches     #promedio de la perdida en todos los lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5590b22",
   "metadata": {},
   "source": [
    "De  forma  predeterminada,  la  función  calc_loss_batch  itera  sobre  todos  los  lotes  de  un  cargador  de  datos  determinado,  acumula  la  pérdida  en  la  variable  total_loss  y,  a  continuación,  calcula  y  promedia  la  pérdida  sobre  el  número  total  de  lotes.  Como  alternativa, se puede especificar  un  número  menor  de  lotes  mediante  num_batches  para  agilizar  la  evaluación  durante  el  entrenamiento  del  modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b9348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583796183268\n",
      "Validation loss: 11.025263786315918\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "with torch.no_grad(): #deshabilitar el seguimiento de gradiente ya que no se esta entrenando\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552d0e8",
   "metadata": {},
   "source": [
    "Los  valores  de  pérdida  son  relativamente  altos  porque  el  modelo  aún  no  se  ha  entrenado.  A  modo  de  comparación,  la  pérdida  se  aproxima  a  cero  si  el  modelo  aprende  a  generar  los  siguientes  tokens  a  medida  que  aparecen  en  los  conjuntos  de  entrenamiento  y  validación.\n",
    "\n",
    "Ahora  que  se tiene  una  forma  de  medir  la  calidad  del  texto  generado,  en  la  siguiente  sección  se entrenará  al  LLM  para  reducir  esta  pérdida  para  que  sea  mejor  en  la  generación  de  texto.\n",
    "\n",
    "![Texto alternativo](./imgs/5.10.png)\n",
    "\n",
    "[Entrenamiento de un LLM](./2_entrenando_llm.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
