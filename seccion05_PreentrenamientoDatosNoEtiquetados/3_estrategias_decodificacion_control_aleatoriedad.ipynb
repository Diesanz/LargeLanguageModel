{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765bc53f",
   "metadata": {},
   "source": [
    "# Estrategias de decodificación para controlar la aleatoriedad\n",
    "\n",
    "En esta sección se abordarán las   estrategias  de  generación  de  texto  (también  llamadas  estrategias  de  decodificación)  para  \n",
    "generar  texto  más  original.  \n",
    "\n",
    "- Primero,  revisar  brevemente  la  función  generate_text_simple  del ña sección anterior,  que se usa   dentro  de  generate_and_print_sample \n",
    "- Después,  se abordarán  dos  técnicas:   \n",
    "    - escalado  de  temperatura  \n",
    "    - el  muestreo  topk,  para  mejorar  esta  función\n",
    "\n",
    "Ahora se volverá a cargar el modelo y entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23676cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtiene la ruta de la carpeta principal del proyecto (subiendo un nivel desde seccion05)\n",
    "ruta_proyecto_principal = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Añade esta ruta a la lista de lugares donde Python busca módulos\n",
    "if ruta_proyecto_principal not in sys.path:\n",
    "    sys.path.append(ruta_proyecto_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0ffb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.063, Val loss 9.945\n",
      "Ep 1 (Step 000005): Train loss 8.156, Val loss 8.334\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.556, Val loss 7.037\n",
      "Ep 2 (Step 000015): Train loss 5.927, Val loss 6.580\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.824, Val loss 6.464\n",
      "Ep 3 (Step 000025): Train loss 5.381, Val loss 6.365\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.598, Val loss 6.250\n",
      "Ep 4 (Step 000035): Train loss 5.028, Val loss 6.267\n",
      "Every effort moves you of the picture.      \"I                \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 4.076, Val loss 6.136\n",
      "Every effort moves you know the                          \"Oh, and the fact a little the latter the honour his pictures--and it's--I he had\n",
      "Ep 6 (Step 000045): Train loss 4.005, Val loss 6.138\n",
      "Ep 6 (Step 000050): Train loss 2.871, Val loss 6.130\n",
      "Every effort moves you know the fact, and pushed one of the to the fact of the last word.        \"Oh, and I was his pictures--I had the donkey. I had the donkey. \"I looked. \n",
      "Ep 7 (Step 000055): Train loss 3.065, Val loss 6.149\n",
      "Ep 7 (Step 000060): Train loss 2.057, Val loss 6.116\n",
      "Every effort moves you know,\" was not that the picture.  I-chairs forward. \"There: \"Yes, and!  \"I didn't say, and I was a little. \"I he was his pictures--because he was his pictures\n",
      "Ep 8 (Step 000065): Train loss 1.958, Val loss 6.159\n",
      "Ep 8 (Step 000070): Train loss 1.339, Val loss 6.268\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word. Gisburn's an awful simpleton, and Mrs. I remember getting off a prod, as once one had been the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.221, Val loss 6.245\n",
      "Ep 9 (Step 000080): Train loss 0.819, Val loss 6.264\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--because he's. The\n",
      "Ep 10 (Step 000085): Train loss 0.574, Val loss 6.377\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from seccion04_ImplementacionGPTGeneracionTexto.gptModel import GPTModel\n",
    "from seccion04_ImplementacionGPTGeneracionTexto.gptConfig124M import GPT_CONFIG_124M\n",
    "from seccion02_TrabajarDatosTexto.dataloader_v1 import create_dataloader_v1\n",
    "from trainModelSimple import train_model_simple\n",
    "\n",
    "GPT_CONFIG_124M[\"context_length\"] = 256\n",
    "\n",
    "file_path = \"../txt/The_Verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[: split_idx]\n",
    "val_data = text_data[split_idx :]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #el método parameters() devuelve todos los pesos entrenables del modelo\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283e431",
   "metadata": {},
   "source": [
    "Se comienza transfiriendo  el  modelo  de  vuelta  de  la  GPU  a  la  CPU,  ya  que  la  inferencia  con  un  modelo  relativamente  pequeño  no  requiere  una  GPU.  Además,  después  del  entrenamiento,  se pone en  modo  de  evaluación  para  desactivar  componentes  aleatorios  como  la  pérdida  de  datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880475aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229df08",
   "metadata": {},
   "source": [
    "A continuación se conecta la instancia GPTModel (modelo) a la función **generate_text_simple**, que utiliza el LLM para generar un token a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49b4659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "He laughed again, and threw back his head to\n"
     ]
    }
   ],
   "source": [
    "from seccion04_ImplementacionGPTGeneracionTexto.generateTextSimple import generate_text_simple\n",
    "from trainModelSimple import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56ee40",
   "metadata": {},
   "source": [
    "El  token  generado  se  selecciona  en  cada  paso  de  generación  correspondiente  al  puntaje  de  probabilidad  más  grande  entre  todos  los  tokens  del  vocabulario.\n",
    "Las  siguientes  subsecciones  se introducen  dos  conceptos  para  controlar  la  aleatoriedad  y Diversidad  del  texto  generado:  escalamiento  de  temperatura  y  muestreo  top-k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb672722",
   "metadata": {},
   "source": [
    "### 3.1 Escala de temperatura\n",
    "\n",
    "El  escalamiento  de  temperatura,  una  técnica  que  agrega  un  proceso  de  selección  probabilística  a  la  tarea  de  generación  del  próximo  token.\n",
    "\n",
    "Anteriormente,  dentro  de  la  función  generate_text_simple ,  siempre  se muestreába  el  token  con  la  mayor  probabilidad  como  el  siguiente  token  usando  torch.argmax,  también  conocido  como  decodificación  voraz.  Para generar  texto  con  mayor  variedad,  se puede  reemplazar  argmax  con  una  función  que  muestrea  de  una  distribución  de  probabilidad  (en  este  caso,  las  puntuaciones  de  probabilidad  que  genera  el  LLM  para  cada  entrada  de  vocabulario  en  cada  paso  de  generación  de  tokens).\n",
    "\n",
    "Para  ilustrar  el  muestreo  probabilístico  con  un  ejemplo  concreto,  analiza  brevemente  el proceso  de  generación  del  siguiente  token  que  utiliza  un  vocabulario  muy  pequeño  para  fines  ilustrativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18010de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f4ea7",
   "metadata": {},
   "source": [
    "A continuación, suponer que al LLm se le da el contexto **\"every effort moves you** y genera los siguiente logits de siguiente token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8c14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f256fb",
   "metadata": {},
   "source": [
    "Como ya se ha discutido  dentro  de  generate_text_simple se convierten  los  logits  en  probabilidades  a  través  de  la  función  softmax y se obtienen el ID del token correspondiente al token generado a través de la función argmaxm que luego se puede mapear nuevamente al texto a tarves del vocabulario inverso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d583f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb4a75",
   "metadata": {},
   "source": [
    "La salida impresa es \"forward\" (adelante) igual que antes. ¿Qué pasó? La función multinomial muestrea el siguiente token de forma proporcional a su puntuación de probabilidad. En otras palabras, \"forward\" sigue siendo el token más probable y será seleccionado por la función multinomial la mayoría de las veces, pero no siempre. Para ilustrar esto, se implementa una función que repita este muestreo 1000 veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4099fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd460eb2",
   "metadata": {},
   "source": [
    "Como se puede ver en la salida, la palabra **forward** se muestrea la mayor parte del tiempo, pero otros tokens como **closer**, **toward** también se muestran ocasionalmente. Esto significa que si se reemplaza la función argmax con la función multinomial dentro de la función generate_and_print_sample, el LLM a veces generaría textos \"every effort moves you toward\", \"every effortmoves you inches\", y \"every effort moves you closer\" en vez de \"every effortmoves you forward\".\n",
    "\n",
    "Se puede controlar  aún  más  el  proceso  de  distribución  y  selección  a  través  de  un  concepto  llamado  escala  de  temperatura,  donde  la  escala  de  temperatura  es  simplemente  una  descripción  elegante  para  dividir  los  logits  por  un  número  mayor  que  0:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a08849e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
       "         1.0120e-04, 3.5758e-01, 4.0122e-03]),\n",
       " tensor([1.8530e-10, 3.5189e-26, 2.6890e-38, 9.9099e-01, 5.7569e-23, 4.4220e-37,\n",
       "         2.9718e-38, 9.0133e-03, 2.8514e-22]),\n",
       " tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "#Las  temperaturas  superiores  a  1  dan  como  resultado  probabilidades  de  tokens  distribuidas  de  manera  más  uniforme.\n",
    "#Y  las  temperaturas  menores  a  1  darán  como  resultado  temperaturas  más  confiables  (más  nítidas  o  con  más  picos).\n",
    "#Distribuciones.  Ilustremos  esto  representando  gráficamente  las  probabilidades  originales  junto  con  las  probabilidades  \n",
    "#escaladas  con  diferentes  valores  de  temperatura:  \n",
    "\n",
    "temperatures  =  [1,  0.1,  5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "scaled_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d5d6fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "                    bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6849b5",
   "metadata": {},
   "source": [
    "Una  temperatura  de  1  divide  los  logits  entre  1  antes  de  pasarlos  a  la  función  softmax  para  calcular  las  puntuaciones  de  probabilidad.  En  otras  palabras,  usar  una  temperatura  de  1  equivale  a  no  usar  escala  de  temperatura.  En  este  caso,  los  tokens  se  seleccionan  con  una  probabilidad  igual  a  las  puntuaciones  de  probabilidad  softmax  originales  mediante  la  función  de  muestreo  multinomial  de  PyTorch.\n",
    "\n",
    "Además,  como  se  observa  en  la  Figura,  aplicar  temperaturas  muy  pequeñas,  como  0,1,  dará  como  resultado  distribuciones  más  nítidas,  de  modo  que  el  comportamiento  de  la  función  multinomial  selecciona  el  token  más  probable  (en  este  caso:  \"forward\")  casi  el  100  %  de  las  veces,  acercándose  al  comportamiento  de  la  función  argmax.  Por  el  contrario,  una  temperatura  de  5  da  como  resultado  una  distribución  más  uniforme,  donde  se  seleccionan  otros  tokens  con  mayor  frecuencia.  Esto  puede  añadir  más  variedad  a  los  textos  generados,  pero  también  suele  generar  texto  sin  sentido.  Por  ejemplo,  usar  una  temperatura  de  5  da  como  resultado  textos  como  \"every effort moves you pizza\"  aproximadamente  el  4  %  deel  tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1ceb8",
   "metadata": {},
   "source": [
    "### 3.2 Muestreo de lo mejores k (top-k)\n",
    "\n",
    "En la sección anterior, se implementó un enfoque de muestreo probabilístico combinado con el escalamiento de temperatura para aumentar la diversidad de los resultados generados. Se observó que valores de temperatura más altos aplanan la distribución de probabilidad, resultando en una selección de tokens más uniforme.\n",
    "\n",
    "Este método permite al modelo explorar caminos menos probables, pero potencialmente más interesantes y creativos, ya que reduce la tendencia a seleccionar repetidamente el token más probable.\n",
    "\n",
    "Sin embargo, una desventaja notable de este enfoque es que, al aumentar la aleatoriedad, a veces genera resultados gramaticalmente incorrectos o completamente sin sentido.\n",
    "\n",
    "Para solucionar el problema anterior y mejorar la calidad de la generación, en esta sección se introduce el concepto de muestreo top-k.\n",
    "\n",
    "El objetivo de esta técnica es restringir el proceso de muestreo únicamente a los k tokens más probables. Todos los demás tokens (los menos probables) son excluidos del proceso de selección mediante el enmascaramiento de sus puntuaciones de probabilidad.\n",
    "\n",
    "Al combinar el muestreo top-k con el escalamiento de temperatura, es posible mantener un buen nivel de diversidad, pero limitando las opciones a un conjunto de candidatos que ya son plausibles, mejorando así la coherencia y la calidad general del texto generado.\n",
    "\n",
    "![Texto alternativo](./imgs/5.13.png)\n",
    "\n",
    "El  enfoque  descrito  en  la  Figura  reemplaza  todos  los  logits  no  seleccionados  con  valor  infinito  negativo  ( inf),  de  modo  que  al  calcular  los  valores  softmax,  los  puntajes  de  probabilidad  de  los  tokens  que  no  son  topk  son  0  y  las  probabilidades  restantes  suman  1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb56dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, k=top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac60cd",
   "metadata": {},
   "source": [
    "Posteriormente,  se aplica la  función  where  de  PyTorch  para  establecer  los  valores  logit  de  los  tokens  que  están  por  debajo  del  valor  logit  más  bajo  dentro  de  nuestra  selección  top3  en  infinito  negativo  (-inf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930474c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(condition=next_token_logits < top_logits[-1], #Identifica  logits  menores  al  mínimo  en  el  top  3\n",
    "                        input=torch.tensor(float('-inf')),  #Asigna  inf  a  estos  logits  inferiores\n",
    "                        other=next_token_logits)    #Conserva  los  logits  originales  para  todos  los  demás  tokens\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf88645",
   "metadata": {},
   "source": [
    "Por  último,  se aplica  la  función  softmax  para  convertirlas  en  probabilidades  del  próximo  token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6c64317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae82e9",
   "metadata": {},
   "source": [
    "El  resultado  de  este  enfoque  de  los  3  mejores  son  3  puntuaciones  de  probabilidad  distintas  de  cero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
