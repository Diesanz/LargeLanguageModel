{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f447d",
   "metadata": {},
   "source": [
    "# Ocultación de palabras futuras con atención casual\n",
    "\n",
    "En  esta  sección,  se modifica el  mecanismo  de  autoatención  estándar  para  crear  un  mecanismo  de  atención  causal\n",
    "\n",
    "La  atención  causal,  también  conocida  como  atención  enmascarada,  es  una  forma  especializada  de  autoatención.\n",
    " \n",
    "Restringe  un  modelo  para  que  solo  considere  las  entradas  previas  y  actuales  de  una  secuencia  al  procesar  cualquier  token.  \n",
    "Esto  contrasta  con  el  mecanismo  estándar  de  autoatención,  que  permite  acceder  a  toda  la  secuencia  de  entrada  a  la  vez.\n",
    " \n",
    "En  consecuencia,  al  calcular  los  puntajes  de  atención,  el  mecanismo  de  atención  causal  garantiza  que  el  modelo  solo  \n",
    "tenga  en  cuenta  los  tokens  que  aparecen  en  el  mismo  token  o  antes  que  este  en  la  secuencia.Para  lograr  esto  en  LLM  tipo  GPT,  para  cada  token  procesado,  enmascaramos  el  futuro tokens,  que  vienen  después  del  token  actual  en  el  texto  de  entrada\n",
    "\n",
    "![Texto alternativo](./imgs/3.18.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e681f",
   "metadata": {},
   "source": [
    "## Aplicación  de  una  máscara  de  atención  causal\n",
    "\n",
    "![Texto alternativo](./imgs/3.19.png)\n",
    "\n",
    "Una  forma  de  obtener  la  matriz  de  ponderación  de  atención  enmascarada  en  la  atención  causal  es  aplicar  la  función  softmax  a  los  puntajes  de  atención,  poniendo  a  cero  los  elementos  por  encima  de  la  diagonal  y  normalizando  el  resultado de la matriz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7941efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores =queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out) #A  Reutilice  las  matrices  de  consulta  y  peso  clave  del  objeto  SelfAttention_v2  de  la  sección  anterior  para conveniencia\n",
    "queries = sa_v2.W_query(inputs)                                   \n",
    "#A\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78bd5e",
   "metadata": {},
   "source": [
    "Se puede  implementar  el  paso  2  de  la  Figura   usando  la  función  tril  de  PyTorch  para  crear  una  máscara  donde  \n",
    "los  valores  por  encima  de  la  diagonal  sean  cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd0fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "\n",
    "#Aplicar el enmascaramiento\n",
    "simple_masked = attn_weights * mask_simple\n",
    "print(simple_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf07b63",
   "metadata": {},
   "source": [
    "El  tercer  paso  en  la  Figura es  renormalizar  los  pesos  de  atención  para  que  sumen  1  nuevamente  en  cada  fila.  Podemos  lograr  esto  dividiendo  cada  elemento  de  cada  fila  por  la  suma  de  cada  uno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a4aa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = simple_masked.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = simple_masked / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0bb4a",
   "metadata": {},
   "source": [
    "Cuando  aplicamos  una  máscara  y  luego  renormalizamos  los  pesos  de  atención,  podría  parecer  inicialmente  que  la  \n",
    "información  de  los  tokens  futuros  (que  pretendemos  enmascarar)  aún  podría  influir  en  el  token  actual  porque  sus  valores  \n",
    "son  parte  del  cálculo  de  softmax.\n",
    "\n",
    "Sin  embargo,  la  idea  clave  es  que  cuando  renormalizamos  los  pesos  de  atención  después  del  enmascaramiento,  lo  que  \n",
    "estamos  haciendo  esencialmente  es  recalcular  el  softmax  sobre  un  subconjunto  más  pequeño  (ya  que  las  posiciones  \n",
    "enmascaradas  no  contribuyen  al  valor  softmax).\n",
    "\n",
    "La  elegancia  matemática  de  softmax  es  que,  a  pesar  de  incluir  inicialmente  todas  las  posiciones  en  el  denominador,  \n",
    "después  de  enmascarar  y  renormalizar,  el  efecto  de  las  posiciones  enmascaradas  se  anula;  no  contribuyen  al  puntaje  \n",
    "de  softmax  de  ninguna  manera  significativa.\n",
    "\n",
    "En  términos  más  simples,  después  del  enmascaramiento  y  la  renormalización,  la  distribución  de  los  pesos  de  atención  \n",
    "es  como  si  se  hubiera  calculado  sólo  entre  las  posiciones  no  enmascaradas  desde  el  principio.\n",
    "Esto  garantiza  que  no  haya  fugas  de  información  de  tokens  futuros  (o  de  otro  modo  enmascarados)  como  pretendíamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a22b0e",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/3.20.png)\n",
    "\n",
    "Una  forma  más  eficiente  de  obtener  la  matriz  de  peso  de  atención  enmascarada  en  la  atención  causal  es  enmascarar  los  puntajes  de  atención  con  valores  infinitos  negativos  antes  de  aplicar  la  función  softmax.\n",
    "\n",
    "La  función  softmax  convierte  sus  entradas  en  una  distribución  de  probabilidad.  Cuando  hay  valores  de  infinito  negativo  (∞)  \n",
    "consecutivos,  la  función  softmax  los  trata  como  cero.probabilidad.  (Matemáticamente,  esto  se  debe  a  que  e ∞se  aproxima  a  0.)\n",
    "\n",
    "Podemos  implementar  este  \"truco\"  de  enmascaramiento  más  eficiente  creando  una  máscara  con  1  arribala  diagonal  y  luego  reemplazar  estos  1  con  valores  negativos  de  infinito  ( inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619e6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) #matriz de unos en la diagonal superior\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45624d61",
   "metadata": {},
   "source": [
    "Ahora  podríamos  usar  los  pesos  de  atención  modificados  para  calcular  los  vectores  de  contexto  mediante  context_vec  =  \n",
    "attn_weights  @  values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95006d",
   "metadata": {},
   "source": [
    "## Enmascaramiento de  pesos  de  atención  adicionales  con  abandono\n",
    "\n",
    "![Texto alternativo](./imgs/3.21.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
