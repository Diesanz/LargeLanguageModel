{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f447d",
   "metadata": {},
   "source": [
    "# Ocultación de palabras futuras con atención casual\n",
    "\n",
    "En  esta  sección,  se modifica el  mecanismo  de  autoatención  estándar  para  crear  un  mecanismo  de  atención  causal\n",
    "\n",
    "La  atención  causal,  también  conocida  como  atención  enmascarada,  es  una  forma  especializada  de  autoatención.\n",
    " \n",
    "Restringe  un  modelo  para  que  solo  considere  las  entradas  previas  y  actuales  de  una  secuencia  al  procesar  cualquier  token.  \n",
    "Esto  contrasta  con  el  mecanismo  estándar  de  autoatención,  que  permite  acceder  a  toda  la  secuencia  de  entrada  a  la  vez.\n",
    " \n",
    "En  consecuencia,  al  calcular  los  puntajes  de  atención,  el  mecanismo  de  atención  causal  garantiza  que  el  modelo  solo  \n",
    "tenga  en  cuenta  los  tokens  que  aparecen  en  el  mismo  token  o  antes  que  este  en  la  secuencia.Para  lograr  esto  en  LLM  tipo  GPT,  para  cada  token  procesado,  enmascaramos  el  futuro tokens,  que  vienen  después  del  token  actual  en  el  texto  de  entrada\n",
    "\n",
    "![Texto alternativo](./imgs/3.18.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e681f",
   "metadata": {},
   "source": [
    "## Aplicación  de  una  máscara  de  atención  causal\n",
    "\n",
    "![Texto alternativo](./imgs/3.19.png)\n",
    "\n",
    "Una  forma  de  obtener  la  matriz  de  ponderación  de  atención  enmascarada  en  la  atención  causal  es  aplicar  la  función  softmax  a  los  puntajes  de  atención,  poniendo  a  cero  los  elementos  por  encima  de  la  diagonal  y  normalizando  el  resultado de la matriz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7941efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")\n",
    "\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores =queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out) #A Reutilice  las  matrices  de  consulta  y  peso  clave  del  objeto  SelfAttention_v2  de  la  sección  anterior  para conveniencia\n",
    "queries = sa_v2.W_query(inputs)                                   \n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78bd5e",
   "metadata": {},
   "source": [
    "Se puede  implementar  el  paso  2  de  la  Figura   usando  la  función  tril  de  PyTorch  para  crear  una  máscara  donde  \n",
    "los  valores  por  encima  de  la  diagonal  sean  cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbd0fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "\n",
    "#Aplicar el enmascaramiento\n",
    "simple_masked = attn_weights * mask_simple\n",
    "print(simple_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf07b63",
   "metadata": {},
   "source": [
    "El  tercer  paso  en  la  Figura es  renormalizar  los  pesos  de  atención  para  que  sumen  1  nuevamente  en  cada  fila.  Podemos  lograr  esto  dividiendo  cada  elemento  de  cada  fila  por  la  suma  de  cada  uno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52a4aa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = simple_masked.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = simple_masked / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0bb4a",
   "metadata": {},
   "source": [
    "Cuando  aplicamos  una  máscara  y  luego  renormalizamos  los  pesos  de  atención,  podría  parecer  inicialmente  que  la  \n",
    "información  de  los  tokens  futuros  (que  pretendemos  enmascarar)  aún  podría  influir  en  el  token  actual  porque  sus  valores  \n",
    "son  parte  del  cálculo  de  softmax.\n",
    "\n",
    "Sin  embargo,  la  idea  clave  es  que  cuando  renormalizamos  los  pesos  de  atención  después  del  enmascaramiento,  lo  que  \n",
    "estamos  haciendo  esencialmente  es  recalcular  el  softmax  sobre  un  subconjunto  más  pequeño  (ya  que  las  posiciones  \n",
    "enmascaradas  no  contribuyen  al  valor  softmax).\n",
    "\n",
    "La  elegancia  matemática  de  softmax  es  que,  a  pesar  de  incluir  inicialmente  todas  las  posiciones  en  el  denominador,  \n",
    "después  de  enmascarar  y  renormalizar,  el  efecto  de  las  posiciones  enmascaradas  se  anula;  no  contribuyen  al  puntaje  \n",
    "de  softmax  de  ninguna  manera  significativa.\n",
    "\n",
    "En  términos  más  simples,  después  del  enmascaramiento  y  la  renormalización,  la  distribución  de  los  pesos  de  atención  \n",
    "es  como  si  se  hubiera  calculado  sólo  entre  las  posiciones  no  enmascaradas  desde  el  principio.\n",
    "Esto  garantiza  que  no  haya  fugas  de  información  de  tokens  futuros  (o  de  otro  modo  enmascarados)  como  pretendíamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a22b0e",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/3.20.png)\n",
    "\n",
    "Una  forma  más  eficiente  de  obtener  la  matriz  de  peso  de  atención  enmascarada  en  la  atención  causal  es  enmascarar  los  puntajes  de  atención  con  valores  infinitos  negativos  antes  de  aplicar  la  función  softmax.\n",
    "\n",
    "La  función  softmax  convierte  sus  entradas  en  una  distribución  de  probabilidad.  Cuando  hay  valores  de  infinito  negativo  (∞)  \n",
    "consecutivos,  la  función  softmax  los  trata  como  cero.probabilidad.  (Matemáticamente,  esto  se  debe  a  que  e ∞se  aproxima  a  0.)\n",
    "\n",
    "Podemos  implementar  este  \"truco\"  de  enmascaramiento  más  eficiente  creando  una  máscara  con  1  arribala  diagonal  y  luego  reemplazar  estos  1  con  valores  negativos  de  infinito  ( inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "619e6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) #matriz de unos en la diagonal superior\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45624d61",
   "metadata": {},
   "source": [
    "Ahora  podríamos  usar  los  pesos  de  atención  modificados  para  calcular  los  vectores  de  contexto  mediante  context_vec  =  \n",
    "attn_weights  @  values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95006d",
   "metadata": {},
   "source": [
    "## Enmascaramiento de  pesos  de  atención  adicionales  con  abandono\n",
    "\n",
    "La  deserción  en  el  aprendizaje  profundo  es  una  técnica  que  ignora  unidades  de  capa  oculta  seleccionadas  aleatoriamente  \n",
    "durante  el  entrenamiento,  descartándolas.  Este  método  ayuda  a  prevenir  el  sobreajuste,  ya  que  garantiza  que  un  modelo  no  \n",
    "dependa  excesivamente  de  ningún  conjunto  específico  de  unidades  de  capa  oculta.  Es  importante  destacar  que  la  deserción  \n",
    "solo  se  utiliza  durante  el  entrenamiento.y  se  desactiva  posteriormente.\n",
    "\n",
    "En  la  arquitectura  del  transformador,  incluidos  modelos  como  GPT,  la  pérdida  de  atención  en  el  mecanismo  se  aplica  \n",
    "típicamente  en  dos  áreas  específicas:  después  de  calcular  los  puntajes  de  atención  o  después  de  aplicar  los  pesos  de  \n",
    "atención  a  los  vectores  de  valores.\n",
    "\n",
    "Se aplicará la  máscara  de  abandono  después  de  calcular  los  pesos  de  atención.\n",
    "\n",
    "![Texto alternativo](./imgs/3.21.png)\n",
    "\n",
    "Se utilizará  una  tasa  de  abandono  del  50  %,  lo  que  implica  enmascarar  la  mitad  de  los  pesos  de  atención. \n",
    "Se aplica primero  la  implementación  de  abandono  de  PyTorch  a  un  tensor  de  6×6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20ea6a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dropout = nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "\n",
    "print(example)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09283e2",
   "metadata": {},
   "source": [
    "La  mitad  de  los  elementos  de  la  matriz  se  establecen  aleatoriamente  en  cero.  Para  compensar  la  reducción  de  elementos  activos,  los  valores  de  los  elementos  restantes  de  la  matriz  se  escalan  por  un  factor  de  1/0,5  =  2.  Este  escalamiento  es  crucial  para  mantener  el  equilibrio  general  de  las  ponderaciones  de  atención,  garantizando  que  la  influencia  promedio  del  mecanismo  de  atención  se  mantenga  constante  durante  las  fases  de  entrenamiento  e  inferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195c5a7",
   "metadata": {},
   "source": [
    "## Implementación  de  una  clase  de  atención  causal  compacta\n",
    "\n",
    "Incorporación de las modificaciones  de  atención  causal  y  abandono  en  la  clase  de  Python  SelfAttention  que se han desarrollado.  Esta  clase  servirá  como  plantilla  para  desarrollar  la  atención  multicabeza  en  la  siguiente  sección,  que  es  la  última  clase  de  atención  que  implementamos  en  este  capítulo.\n",
    "\n",
    "Antes  de  comenzar,  una  cosa  más  es  asegurarnos  de  que  el  código  pueda  manejar  lotes  que  constan  \n",
    "de  más  de  una  entrada  para  que  la  clase  CausalAttention  admita  las  salidas  por  lotes  producidas  por  el  cargador  \n",
    "de  datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5ac4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)  \n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f592f3f",
   "metadata": {},
   "source": [
    "La  siguiente  clase  CausalAttention  es  similar  a  la  clase  SelfAttention,  excepto  que  ahora  se agregan  los  componentes  de  abandono  y  máscara  causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de1871a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout,  qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  #Capa de abandono\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                            \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)              #transponer las dimensiones 1 y 2,manteniendo la dimension del lote en la primera dimension (0) \n",
    "        attn_scores.masked_fill_(                                 #las operaciones con guion dinal sen realiza en el lugar lo que evita copias de meoria inecesarias\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791eb73",
   "metadata": {},
   "source": [
    "Ahora  se ha  añadido  una  llamada  a  self.register_buffer()  en  el método  __init__ .  El  uso  de  register_buffer  en  PyTorch  no  es  estrictamente  necesario  para  todos  los  casos  de  uso,  pero  ofrece  varias  ventajas.  Por  ejemplo,  al  usar  la  clase  CausalAttention  en  nuestro  LLM,  los  búferes  se  mueven  automáticamente  al  dispositivo  correspondiente  (CPU  o  GPU)  junto  con  nuestro  modelo,  lo  cual  será  relevante  al  entrenar  el  LLM  . No es necesario aegurarse que los tensores estén en el mismo dispositivo que los parámetros del modelo. Evita discrepacias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb3aba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb302af",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/3.22.png)\n",
    "\n",
    "[Ampliación de la atención de una sola cabeza a la atención de múltiples cabezas](./6_atencion_multicabecera.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
