{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd914a6",
   "metadata": {},
   "source": [
    "# Implementación de la autoatención con pesos entrenables\n",
    "\n",
    "En  esta  sección,  implementamos  el  mecanismo  de  autoatención  utilizado  en  la  arquitectura  original  del  transformador,  los  modelos  GPT  y  la  mayoría  de  los  demás  LLM  populares.  \n",
    "\n",
    "Este  mecanismo  también  se  denomina  atención  escalar  de  producto  escalar.\n",
    "\n",
    "![Texto alternativo](./imgs/3.12.png)\n",
    "\n",
    "- La self-attention con pesos entrenables calcula vectores de contexto como combinaciones ponderadas de las entradas.\n",
    "\n",
    "- La diferencia clave respecto a la versión básica: se introducen matrices de pesos entrenables que se ajustan durante el entrenamiento.\n",
    "\n",
    "- Estas matrices permiten que el modelo aprenda a producir buenos vectores de contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b41a9f",
   "metadata": {},
   "source": [
    "## Cálculo de los pesos de atención paso a paso\n",
    "\n",
    "En este apartado se implementa el mecanismo de autoatención introduciendo tres matrices de peso entrenables: `Wq`, `Wk` y `Wv`. Estas matrices se utilizan para proyectar los *tokens* de entrada incrustados `x(i)` en vectores de **consulta (query)**, **clave (key)** y **valor (value)**, respectivamente. \n",
    "\n",
    "El procedimiento comienza calculando los vectores `q`, `k` y `v` mediante multiplicaciones de la entrada por las correspondientes matrices de peso. De forma análoga a lo visto en la sección anterior, primero se ilustra el cálculo de un único vector de contexto `z(i)`. Posteriormente, el método se generaliza para obtener todos los vectores de contexto de la secuencia.  \n",
    "\n",
    "![Texto alternativo](./imgs/3.13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf26c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Considerar  la  siguiente  oración  de  entrada,  que  ya  ha  sido  incorporada  en  vectores  tridimensionales \n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf63ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] #segundo elemento de la entrada\n",
    "d_in = inputs.shape[1] #tamaño de incrustación de entrada\n",
    "d_out = 2 #tamaño de incrustación de salida\n",
    "\n",
    "#Inicializar tres  matrices  de  peso  Wq ,  Wk  y  Wv\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) #reducir el desorden en las salidas\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "#Calcular los vectores de consulta, clave y valor \n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2) #vector bidimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948111e",
   "metadata": {},
   "source": [
    "Los  parámetros  de  peso  son  los  coeficientes  fundamentales  aprendidos  que  definen  las  \n",
    "conexiones  de  la  red,  mientras  que  los  pesos  de  atención  son  dinámicos  y  específicos  del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6f54b",
   "metadata": {},
   "source": [
    "Aunque el objetivo es z_2, z requiere de los vectores de clave y valor, para todos los elemnentos de entradas, ya que están involucrados en el cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d118804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape:  torch.Size([6, 2])\n",
      "Values shape:  torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "#6 tokens de entrada de un espacion de incrustacion 3D a uno 2D\n",
    "print(\"Keys shape: \", keys.shape)\n",
    "print(\"Values shape: \", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04e220",
   "metadata": {},
   "source": [
    "El segundo paso es calcular los puntuajes de atención\n",
    "\n",
    "![Texto alternativo](./imgs/3.14.png)\n",
    "\n",
    "El  cálculo  de  la  puntuación  de  atención  es  un  cálculo  de  producto  escalar  similar  al  utilizado  en  el  mecanismo  simplificado  de  autoatención.  La  novedad  radica  en  que  no  calculamos  directamente  el  producto  escalar  entre  los  elementos  de  entrada,  sino  que  utilizamos  la  consulta  y  la  clave  obtenidas  al  transformar  las  entradas  mediante  las respectivas  matrices  de  ponderación.\n",
    "\n",
    "Primero calcula el puntuaje de atencio W_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab683cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "query_2 = x_2 @ W_query\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) #resultado no normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be566256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#generalizar calculo a todos los puntuajes de atención \n",
    "attn_score_2 = query_2 @ keys.T\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085e2e3",
   "metadata": {},
   "source": [
    "El tercer paso es pasar de los puntuajes de atentión a los pesos de atención\n",
    "\n",
    "![Texto alternativo](./imgs/3.15.png)\n",
    "\n",
    "Calcular  las  ponderaciones  de  atención  escalando  las  puntuaciones  de  atención  y  utilizando  la  función  softmax  que  utilizamos  anteriormente.  La  diferencia  con  la  función  \n",
    "anterior  radica  en  que  ahora  escalamos  las  puntuaciones  de  atención  dividiéndolas  por  la  raíz  cuadrada  de  la  dimensión  de  incrustación  de  las  claves  (tenga  en  cuenta  que  calcular  la  raíz  cuadrada  equivale  matemáticamente  a  exponenciar  por  0,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f895cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "torch.Size([6, 2]) 2\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k**0.5, dim=-1) #Si la dimension es muy grande, los valores pueden ser grandes y softmaz se vuelvemuy picuda, ara elo se divide por la raiz de d_k\n",
    "print(attn_weights_2)\n",
    "print(keys.shape, d_k)\n",
    "print(d_k ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfcd9a",
   "metadata": {},
   "source": [
    "El paso final es calcular los vectores de contexto\n",
    "\n",
    "![Texto alternativo](./imgs/3.16.png)\n",
    "\n",
    "De la misma forma que se calculo  el  vector  de  contexto  como  una  suma  ponderada  de  los  vectores  de  entrada,  ahora  se calcula  el  vector  de  contexto  como  una  suma  ponderada  de  los  vectores  de  valor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74150b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "contex_vec_2 = attn_weights_2 @ values\n",
    "print(contex_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512b6d9",
   "metadata": {},
   "source": [
    "En la siguiente sección se generalizará este cálculo para z^T\n",
    "\n",
    "### Consulta, Clave y Valor en el Mecanismo de Atención\n",
    "Los términos “consulta” (query), “clave” (key) y “valor” (value) en los mecanismos de atención provienen del ámbito de la recuperación de información y bases de datos, donde se usan conceptos similares para almacenar, buscar y recuperar información.\n",
    "\n",
    "- Consulta (Query):\n",
    "Representa el elemento actual en el que el modelo se centra, por ejemplo, una palabra o token de una oración. Funciona como una consulta de búsqueda en una base de datos y se utiliza para explorar las demás partes de la secuencia de entrada, determinando cuánta atención se les debe prestar.\n",
    "\n",
    "- Clave (Key):\n",
    "Cada elemento de la secuencia de entrada tiene una clave asociada, que actúa como índice o identificador para localizar información relevante. Las claves permiten al modelo encontrar coincidencias con la consulta.\n",
    "\n",
    "- Valor (Value):\n",
    "Representa el contenido real o la representación de los elementos de entrada, similar al valor en un par clave-valor en una base de datos. Una vez que el modelo determina qué claves son más relevantes para la consulta, se recuperan los valores correspondientes, que contienen la información que se utilizará en la salida del mecanismo de atención.\n",
    "\n",
    "Piensa en esto con un ejemplo concreto:\n",
    "\n",
    "Secuencia: `[\"El\", \"gato\", \"come\"]`\n",
    "\n",
    "Cada palabra se transforma en **Q**, **K** y **V**.\n",
    "\n",
    "Supongamos que estamos evaluando la palabra `\"gato\"` (**Q**).\n",
    "\n",
    "Para decidir a qué palabras prestar atención:\n",
    "\n",
    "- `\"gato\"` (**Q**) se compara con cada **Key**: `K(\"El\")`, `K(\"gato\")`, `K(\"come\")`.\n",
    "- La comparación (producto punto o coseno) nos da **qué tan relevante es cada token** respecto a `\"gato\"`.\n",
    "- Luego usamos los valores correspondientes: `V(\"El\")`, `V(\"gato\")`, `V(\"come\")` ponderados por esa relevancia.\n",
    "\n",
    "La analogía con SQL ayuda a entenderlo así:\n",
    "\n",
    "```sql\n",
    "SELECT valor FROM tabla WHERE key = query;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2174d11",
   "metadata": {},
   "source": [
    "## Implementación de una clase compacta de Python con autoatención\n",
    "\n",
    "Para una futura implementación de una LLM resulta útil organizar este código en una clase Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0857b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        querys = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = querys @ keys.T #omega\n",
    "        attn_weights = torch.softmax(attn_scores/(keys.shape[-1]**0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0e776",
   "metadata": {},
   "source": [
    "El método __init__ se inicializan las matrices de pesos entrenables, cada una transaformando cada dimensión d entrada d_in en una dimensión de salida d_out\n",
    "\n",
    "Durante  el  paso  hacia  adelante,  utilizando  el  método  forward,  se claculan  los  puntajes  de  atención  (attn_scores)  multiplicando  consultas y  claves,  normalizando  estos  puntajes  usando  softmax.\n",
    "Finalmente,  se crea  un  vector  de  contexto  ponderando  los  valores  con  estas  atenciones  normalizadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03902233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8be20a",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/3.17.png)\n",
    "\n",
    "En  la  autoatención,  transformamos  los  vectores  de  entrada  de  la  matriz  de  entrada  X  con  las  tres  matrices  de  \n",
    "ponderación:  Wq,  Wk  y  Wv.  A  continuación,  calculamos  la  matriz  de  ponderación  de  la  atención  a  partir  de  las  consultas  (Q)  y  las  claves  (K)  resultantes.  Utilizando  las  ponderaciones  y  valores  de  la  atención  (V),  calculamos  los  vectores  de  contexto  (Z).  (Para  nortemayor  claridad  visual,  en  esta  figura  nos  centramos  en  un  único  texto  de  entrada  con  tokens,  no  en  un  conjunto  de  múltiples  entradas.  \n",
    "\n",
    "Por  consiguiente,  el  tensor  de  entrada  3D  se  simplifica  a  una  matriz  2D  en  este  contexto.  Este  enfoque  permite  una visualización  y  comprensión  más  sencillas  de  los  procesos  involucrados.  Además,  para  mantener  la  coherencia  con  las  figuras  posteriores,  los  valores  de  la  matriz  de  atención  no  representan  las  ponderaciones  reales  de  la  atención)\n",
    "\n",
    "La  autoatención  implica  las  matrices  de  pesos  entrenables  Wq ,  Wk  y  Wv .  Estas  matrices  transforman  los  datos  de  entrada  en  consultas,  claves  y  valores,  componentes  cruciales  del  mecanismo  de  atención.  A  medida  que  el  modelo  se  expone  a  más  datos  \n",
    "durante  el  entrenamiento,  ajusta  estos  pesos  entrenables.\n",
    "\n",
    "Se puede mejorar  aún  más  la  implementación  de  SelfAttention_v1  utilizando  las  capas  nn.Linear  de  PyTorch ,  que  realizan  la  multiplicación  de  matrices  eficazmente  cuando  las  unidades  de  sesgo  están  deshabilitadas.  Además,  una  ventaja  significativa  de  usar  nn.Linear  en  lugar  de  implementar  manualmente  nn.Parameter(torch.rand(...))  es  que  nn.Linear  cuenta  con  un  esquema  de  inicialización de  pesos  optimizado,  lo  que  contribuye  a  un  entrenamiento  del  modelo  más  estable  y  eficaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a161d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores =queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07980a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f360be4",
   "metadata": {},
   "source": [
    "SelfAttention_v1  y  SelfAttention_v2  dan  resultados  diferentes  porque  utilizan  pesos  iniciales  diferentes  para  las  matrices  de  peso,  ya  que  nn.Linear  utiliza  un  esquema  de  inicialización  de  peso  más  sofisticado.\n",
    "\n",
    "nn.Linear  en  SelfAttention_v2  utiliza  un  esquema  de  inicialización  de  pesos  diferente  al  de  nn.Parameter(torch.rand(d_in,  d_out))  en  SelfAttention_v1,  lo  que  provoca  que  ambos  mecanismos  produzcan  resultados  distintos.  Para  comprobar  que  ambas  implementaciones,  SelfAttention_v1  y  SelfAttention_v2,  son  similares,  podemos  transferir  las  matrices  de  pesos  de  un  objeto  SelfAttention_v2  a  un  objeto  SelfAttention_v1,  de  modo  que  ambos  objetos  produzcan  los  mismos  resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be28c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa_v1.W_query.data = sa_v2.W_query.weight.data.T.clone()\n",
    "sa_v1.W_key.data   = sa_v2.W_key.weight.data.T.clone()\n",
    "sa_v1.W_value.data = sa_v2.W_value.weight.data.T.clone()\n",
    "\n",
    "torch.manual_seed(789)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31fe93",
   "metadata": {},
   "source": [
    "[Ocultación de palabras futuras con atención casual](./5_ocultacion_palabras_futuras.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
