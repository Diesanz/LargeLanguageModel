{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd914a6",
   "metadata": {},
   "source": [
    "# Implementación de la autoatención con pesos entrenables\n",
    "\n",
    "En  esta  sección,  implementamos  el  mecanismo  de  autoatención  utilizado  en  la  arquitectura  original  del  transformador,  los  modelos  GPT  y  la  mayoría  de  los  demás  LLM  populares.  \n",
    "\n",
    "Este  mecanismo  también  se  denomina  atención  escalar  de  producto  escalar.\n",
    "\n",
    "![Texto alternativo](./imgs/3.12.png)\n",
    "\n",
    "- La self-attention con pesos entrenables calcula vectores de contexto como combinaciones ponderadas de las entradas.\n",
    "\n",
    "- La diferencia clave respecto a la versión básica: se introducen matrices de pesos entrenables que se ajustan durante el entrenamiento.\n",
    "\n",
    "- Estas matrices permiten que el modelo aprenda a producir buenos vectores de contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b41a9f",
   "metadata": {},
   "source": [
    "## Cálculo de los pesos de atención paso a paso\n",
    "\n",
    "En este apartado se implementa el mecanismo de autoatención introduciendo tres matrices de peso entrenables: `Wq`, `Wk` y `Wv`. Estas matrices se utilizan para proyectar los *tokens* de entrada incrustados `x(i)` en vectores de **consulta (query)**, **clave (key)** y **valor (value)**, respectivamente. \n",
    "\n",
    "El procedimiento comienza calculando los vectores `q`, `k` y `v` mediante multiplicaciones de la entrada por las correspondientes matrices de peso. De forma análoga a lo visto en la sección anterior, primero se ilustra el cálculo de un único vector de contexto `z(i)`. Posteriormente, el método se generaliza para obtener todos los vectores de contexto de la secuencia.  \n",
    "\n",
    "![Texto alternativo](./imgs/3.13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf26c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Considerar  la  siguiente  oración  de  entrada,  que  ya  ha  sido  incorporada  en  vectores  tridimensionales \n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf63ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] #segundo elemento de la entrada\n",
    "d_in = inputs.shape[1] #tamaño de incrustación de entrada\n",
    "d_out = 2 #tamaño de incrustación de salida\n",
    "\n",
    "#Inicializar tres  matrices  de  peso  Wq ,  Wk  y  Wv\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) #reducir el desorden en las salidas\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "#Calcular los vectores de consulta, clave y valor \n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2) #vector bidimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948111e",
   "metadata": {},
   "source": [
    "Los  parámetros  de  peso  son  los  coeficientes  fundamentales  aprendidos  que  definen  las  \n",
    "conexiones  de  la  red,  mientras  que  los  pesos  de  atención  son  dinámicos  y  específicos  del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6f54b",
   "metadata": {},
   "source": [
    "Aunque el objetivo es z_2, z requiere de los vectores de clave y valor, para todos los elemnentos de entradas, ya que están involucrados en el cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d118804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape:  torch.Size([6, 2])\n",
      "Values shape:  torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "#6 tokens de entrada de un espacion de incrustacion 3D a uno 2D\n",
    "print(\"Keys shape: \", keys.shape)\n",
    "print(\"Values shape: \", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04e220",
   "metadata": {},
   "source": [
    "El segundo paso es calcular los puntuajes de atención\n",
    "\n",
    "![Texto alternativo](./imgs/3.14.png)\n",
    "\n",
    "El  cálculo  de  la  puntuación  de  atención  es  un  cálculo  de  producto  escalar  similar  al  utilizado  en  el  mecanismo  simplificado  de  autoatención.  La  novedad  radica  en  que  no  calculamos  directamente  el  producto  escalar  entre  los  elementos  de  entrada,  sino  que  utilizamos  la  consulta  y  la  clave  obtenidas  al  transformar  las  entradas  mediante  las respectivas  matrices  de  ponderación.\n",
    "\n",
    "Primero calcula el puntuaje de atencio W_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab683cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "query_2 = x_2 @ W_query\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) #resultado no normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be566256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#generalizar calculo a todos los puntuajes de atención \n",
    "attn_score_2 = query_2 @ keys.T\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085e2e3",
   "metadata": {},
   "source": [
    "El tercer paso es pasar de los puntuajes de atentión a los pesos de atención\n",
    "\n",
    "![Texto alternativo](./imgs/3.15.png)\n",
    "\n",
    "Calcular  las  ponderaciones  de  atención  escalando  las  puntuaciones  de  atención  y  utilizando  la  función  softmax  que  utilizamos  anteriormente.  La  diferencia  con  la  función  \n",
    "anterior  radica  en  que  ahora  escalamos  las  puntuaciones  de  atención  dividiéndolas  por  la  raíz  cuadrada  de  la  dimensión  de  incrustación  de  las  claves  (tenga  en  cuenta  que  calcular  la  raíz  cuadrada  equivale  matemáticamente  a  exponenciar  por  0,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f895cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "torch.Size([6, 2]) 2\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k**0.5, dim=-1) #Si la dimension es muy grande, los valores pueden ser grandes y softmaz se vuelvemuy picuda, ara elo se divide por la raiz de d_k\n",
    "print(attn_weights_2)\n",
    "print(keys.shape, d_k)\n",
    "print(d_k ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfcd9a",
   "metadata": {},
   "source": [
    "El paso final es calcular los vectores de contexto\n",
    "\n",
    "![Texto alternativo](./imgs/3.16.png)\n",
    "\n",
    "De la misma forma que se calculo  el  vector  de  contexto  como  una  suma  ponderada  de  los  vectores  de  entrada,  ahora  se calcula  el  vector  de  contexto  como  una  suma  ponderada  de  los  vectores  de  valor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74150b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "contex_vec_2 = attn_weights_2 @ values\n",
    "print(contex_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512b6d9",
   "metadata": {},
   "source": [
    "En la siguiente sección se generalizará este cálculo para z^T\n",
    "\n",
    "### Consulta, Clave y Valor en el Mecanismo de Atención\n",
    "Los términos “consulta” (query), “clave” (key) y “valor” (value) en los mecanismos de atención provienen del ámbito de la recuperación de información y bases de datos, donde se usan conceptos similares para almacenar, buscar y recuperar información.\n",
    "\n",
    "- Consulta (Query):\n",
    "Representa el elemento actual en el que el modelo se centra, por ejemplo, una palabra o token de una oración. Funciona como una consulta de búsqueda en una base de datos y se utiliza para explorar las demás partes de la secuencia de entrada, determinando cuánta atención se les debe prestar.\n",
    "\n",
    "- Clave (Key):\n",
    "Cada elemento de la secuencia de entrada tiene una clave asociada, que actúa como índice o identificador para localizar información relevante. Las claves permiten al modelo encontrar coincidencias con la consulta.\n",
    "\n",
    "- Valor (Value):\n",
    "Representa el contenido real o la representación de los elementos de entrada, similar al valor en un par clave-valor en una base de datos. Una vez que el modelo determina qué claves son más relevantes para la consulta, se recuperan los valores correspondientes, que contienen la información que se utilizará en la salida del mecanismo de atención.\n",
    "\n",
    "Piensa en esto con un ejemplo concreto:\n",
    "\n",
    "Secuencia: `[\"El\", \"gato\", \"come\"]`\n",
    "\n",
    "Cada palabra se transforma en **Q**, **K** y **V**.\n",
    "\n",
    "Supongamos que estamos evaluando la palabra `\"gato\"` (**Q**).\n",
    "\n",
    "Para decidir a qué palabras prestar atención:\n",
    "\n",
    "- `\"gato\"` (**Q**) se compara con cada **Key**: `K(\"El\")`, `K(\"gato\")`, `K(\"come\")`.\n",
    "- La comparación (producto punto o coseno) nos da **qué tan relevante es cada token** respecto a `\"gato\"`.\n",
    "- Luego usamos los valores correspondientes: `V(\"El\")`, `V(\"gato\")`, `V(\"come\")` ponderados por esa relevancia.\n",
    "\n",
    "La analogía con SQL ayuda a entenderlo así:\n",
    "\n",
    "```sql\n",
    "SELECT valor FROM tabla WHERE key = query;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
