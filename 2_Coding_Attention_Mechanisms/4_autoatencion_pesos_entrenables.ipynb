{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd914a6",
   "metadata": {},
   "source": [
    "# Implementación de la autoatención con pesos entrenables\n",
    "\n",
    "En  esta  sección,  implementamos  el  mecanismo  de  autoatención  utilizado  en  la  arquitectura  original  del  transformador,  los  modelos  GPT  y  la  mayoría  de  los  demás  LLM  populares.  \n",
    "\n",
    "Este  mecanismo  también  se  denomina  atención  escalar  de  producto  escalar.\n",
    "\n",
    "![Texto alternativo](./imgs/3.12.png)\n",
    "\n",
    "- La self-attention con pesos entrenables calcula vectores de contexto como combinaciones ponderadas de las entradas.\n",
    "\n",
    "- La diferencia clave respecto a la versión básica: se introducen matrices de pesos entrenables que se ajustan durante el entrenamiento.\n",
    "\n",
    "- Estas matrices permiten que el modelo aprenda a producir buenos vectores de contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b41a9f",
   "metadata": {},
   "source": [
    "## Cálculo de los pesos de atención paso a paso\n",
    "\n",
    "En este apartado se implementa el mecanismo de autoatención introduciendo tres matrices de peso entrenables: `Wq`, `Wk` y `Wv`. Estas matrices se utilizan para proyectar los *tokens* de entrada incrustados `x(i)` en vectores de **consulta (query)**, **clave (key)** y **valor (value)**, respectivamente. \n",
    "\n",
    "El procedimiento comienza calculando los vectores `q`, `k` y `v` mediante multiplicaciones de la entrada por las correspondientes matrices de peso. De forma análoga a lo visto en la sección anterior, primero se ilustra el cálculo de un único vector de contexto `z(i)`. Posteriormente, el método se generaliza para obtener todos los vectores de contexto de la secuencia.  \n",
    "\n",
    "![Texto alternativo](./imgs/3.13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf26c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Considerar  la  siguiente  oración  de  entrada,  que  ya  ha  sido  incorporada  en  vectores  tridimensionales \n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     \n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     \n",
    "    [0.77, 0.25, 0.10], # one      \n",
    "    [0.05, 0.80, 0.55]] # step     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf63ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] #segundo elemento de la entrada\n",
    "d_in = inputs.shape[1] #tamaño de incrustación de entrada\n",
    "d_out = 2 #tamaño de incrustación de salida\n",
    "\n",
    "#Inicializar tres  matrices  de  peso  Wq ,  Wk  y  Wv\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) #reducir el desorden en las salidas\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "#Calcular los vectores de consulta, clave y valor \n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2) #vector bidimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948111e",
   "metadata": {},
   "source": [
    "Los  parámetros  de  peso  son  los  coeficientes  fundamentales  aprendidos  que  definen  las  \n",
    "conexiones  de  la  red,  mientras  que  los  pesos  de  atención  son  dinámicos  y  específicos  del contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6f54b",
   "metadata": {},
   "source": [
    "Aunque el objetivo es z_2, z requiere de los vectores de clave y valor, para todos los elemnentos de entradas, ya que están involucrados en el cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape:  torch.Size([6, 2])\n",
      "Values shape:  torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "#6 tokens de entrada de un espacion de incrustacion 3D a uno 2D\n",
    "print(\"Keys shape: \", keys.shape)\n",
    "print(\"Values shape: \", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04e220",
   "metadata": {},
   "source": [
    "El segundo paso es calcular los puntuajes de atención\n",
    "\n",
    "![Texto alternativo](./imgs/3.14.png)\n",
    "\n",
    "El  cálculo  de  la  puntuación  de  atención  es  un  cálculo  de  producto  escalar  similar  al  utilizado  en  el  mecanismo  simplificado  de  autoatención.  La  novedad  radica  en  que  no  calculamos  directamente  el  producto  escalar  entre  los  elementos  de  entrada,  sino  que  utilizamos  la  consulta  y  la  clave  obtenidas  al  transformar  las  entradas  mediante  las respectivas  matrices  de  ponderación.\n",
    "\n",
    "Primero calcula el puntuaje de atencio W_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab683cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "query_2 = x_2 @ W_query\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) #resultado no normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be566256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#generalizar calculo a todos los puntuajes de atención \n",
    "attn_score_2 = query_2 @ keys.T\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085e2e3",
   "metadata": {},
   "source": [
    "El tercer paso es pasar de los puntuajes de atentión a los pesos de atención\n",
    "\n",
    "![Texto alternativo](./imgs/3.15.png)\n",
    "\n",
    "Calcular  las  ponderaciones  de  atención  escalando  las  \n",
    "puntuaciones  de  atención  y  utilizando  la  función  softmax  que  utilizamos  anteriormente.  La  diferencia  con  la  función  \n",
    "anterior  radica  en  que  ahora  escalamos  las  puntuaciones  de  atención  dividiéndolas  por  la  raíz  cuadrada  de  la  \n",
    "dimensión  de  incrustación  de  las  claves  (tenga  en  cuenta  que  calcular  la  raíz  cuadrada  equivale  matemáticamente  a  \n",
    "exponenciar  por  0,5)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
