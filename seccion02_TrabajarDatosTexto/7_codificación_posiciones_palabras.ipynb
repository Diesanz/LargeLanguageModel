{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ca0b7d",
   "metadata": {},
   "source": [
    "# Codificación de posiciones de palabras\n",
    " \n",
    "Convertir  los  ID  de  token  en  una  representación  vectorial  continua,  las  llamadas  incrustaciones  de  token.  En  principio,  esta  es  una  entrada  adecuada  para  un  LLM.  Sin  embargo,  una  pequeña  desventaja  de  los  LLM  es  que  su  mecanismo  de  autoatención,  no  tiene  noción  de  posición  ni  orden  para  los  tokens  dentro  de  una  secuencia.\n",
    "\n",
    "El  mismo  ID  de  token  siempre  se  asigna  a  la  misma  representación  vectorial,  independientemente  de  dónde  se  posicione  el  ID  de  token  en  la  secuencia  de  entrada.\n",
    "\n",
    "![Texto alternativo](./imgs/2.15.png)\n",
    "\n",
    "En  principio,  la  incrustación  determinista  e  independiente  de  la  posición  del  ID  del  token  es  beneficiosa  para  la  reproducibilidad.  Sin  embargo,  dado  que  el  mecanismo  de  autoatención  de  los  LLM  también  es  independiente  de  la  posición,  resulta  útil  inyectar  información  de  posición  adicional  en  el  LLM.\n",
    "Las  incrustaciones  posicionales  absolutas  se  asocian  directamente  con  posiciones  específicas  en  una  secuencia.  Para  cada  posición  en  la  secuencia  de  entrada,  se  añade  una  incrustación  única  a  la  incrustación  del  token  para  indicar  su  ubicación  exacta.  Por  ejemplo,  el  primer  token  tendrá  una  incrustación  posicional  específica,  el  segundo  token  otra  incrustación  distinta,  y  así  sucesivamente.\n",
    "\n",
    "![Texto alternativo](./imgs/2.16.png)\n",
    "\n",
    "Las incrustaciones posicionales pueden ser absolutas o relativas: las absolutas indican la posición exacta de cada token, mientras que las relativas se enfocan en la distancia entre ellos, lo que favorece la generalización a secuencias de distinta longitud. OpenAI usa incrustaciones absolutas optimizadas durante el entrenamiento, a diferencia de las fijas del Transformer original. Se crean incrustaciones iniciales con vectores de 256 dimensiones (menores que las 12 288 de GPT-3) a partir de tokens generados con el tokenizador BPE de 50 257 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b106454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab_tam = 50257\n",
    "salida_dim = 256\n",
    "embedding_layer = torch.nn.Embedding(vocab_tam, salida_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0de01",
   "metadata": {},
   "source": [
    "Si  tomamos  muestras  de  datos  del  cargador  de  datos, Incruste  cada  token  de  cada  lote  en  un  vector  de  256  dimensiones.\n",
    "Si  tenemos  un  tamaño  de  lote  de  8 con  cuatro  tokens  cada  uno,  el  resultado  será  un  tensor  de  8  x  4  x  256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d7b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "with open(\"../txt/The_Verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_lenght, stride):\n",
    "        self.input_ids = []\n",
    "        self.labels_ids = []\n",
    "\n",
    "        tokens_ids = tokenizer.encode(txt) #Tokenzar el texto completo\n",
    "\n",
    "        for i in range(0, len(tokens_ids) - max_lenght, stride): #Uso de una  ventana  deslizante  para  dividir  el  libro  en  secuencias  superpuestas  de  longitud  máxima\n",
    "            self.input_ids.append(torch.tensor(tokens_ids[i:i+max_lenght]))\n",
    "            self.labels_ids.append(torch.tensor(tokens_ids[i+1:i+max_lenght+1]))\n",
    "\n",
    "    def __len__(self): #Devuelve  el  número  total  de  filas  en  el  conjunto  de  datos\n",
    "        return len(self.labels_ids)\n",
    "    \n",
    "    def __getitem__(self, index): #Devuelve  una  sola  fila  del  conjunto  de  datos\n",
    "        return self.input_ids[index], self.labels_ids[index]\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "       stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "   tokenizer = tiktoken.get_encoding(\"gpt2\")         #inicializar toenizer          \n",
    "   dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)    #Crear dataset\n",
    "   dataloader = DataLoader( \n",
    "       dataset,\n",
    "       batch_size=batch_size,\n",
    "       shuffle=shuffle,\n",
    "       drop_last=drop_last,                                  \n",
    "       num_workers=num_workers                                   \n",
    "   )\n",
    "   return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50ba422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids tokens:  tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Forma entrada:  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, labels = next(data_iter)\n",
    "print(\"Ids tokens: \", inputs)\n",
    "print(\"Forma entrada: \", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cceb693",
   "metadata": {},
   "source": [
    "Ahora  usemos  la  capa  de  incrustación  para  incrustar  estos  identificadores  de  token  en  un  modelo  de  256  dimensiones de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee31c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "tokens_embeddings = embedding_layer(inputs)\n",
    "print(tokens_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea3165c",
   "metadata": {},
   "source": [
    "Para  un  enfoque  de  incrustación  absoluta  de  un  modelo  GPT,  solo  necesitamos  crear  otroa capa  de  incrustación  que  tiene  la  misma  dimensión  que  token_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36edc5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, salida_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b350136",
   "metadata": {},
   "source": [
    "Como se puede ver,  el  tensor  de  incrustación  posicional  consta  de  cuatro  vectores  de  256  dimensiones.\n",
    "Ahora  podemos  agregarlos  directamente  a  las  incrustaciones  de  tokens,  donde  PyTorch  agregará  el  tensor  pos_embeddings  de  4x256  dimensiones  a  cada  tensor  de  incrustación  de  tokens  de  4x256  dimensiones  en  cada  uno  de  los  8  lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a1402bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = tokens_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2781e6a",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/2.17.png)\n",
    "\n",
    "Como  parte  del  proceso  de  procesamiento  de  entrada,  el  texto  de  entrada  se  divide  primero  en  tokens  individuales.  Estos  tokens  se  convierten  en  identificadores  de  token  mediante  un  vocabulario.  Los  identificadores  de  token  se  convierten  en  vectores  de  incrustación  a  los  que  se  añaden  incrustaciones  posicionales  de  tamaño  similar,  lo  que  da  como  resultado  incrustaciones  de  entrada  que  se  utilizan  como  entrada  para  las  capas  principales  de  LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
