{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60001c13",
   "metadata": {},
   "source": [
    "# Codificación de una arquitectura LLM\n",
    "\n",
    "Los  LLM,  como  GPT  (que  significa  Transformador  Generativo  Preentrenado),  son  grandes  arquitecturas  de  redes  neuronales  profundas  diseñadas  para  generar  texto  nuevo,  una  palabra  (o  token)  a  la  vez. Sin  embargo,  a  pesar  de  su  tamaño,  la  arquitectura  del  modelo  es  menos  compleja  de  lo  que  se  podría  pensar,  ya  que  muchos  de  sus  componentes  se  repiten.\n",
    "\n",
    "![Texto alternativo](./imgs/4.2.png)\n",
    "\n",
    "En secciones anteriores se utilizaron dimensiones  de  incrustación  más  pequeñas  para  simplificar,  garantizando  que  los  conceptos  y  ejemplos  cupieran  cómodamente  en  una  sola  página.  Ahora,  se ampliará  el  tamaño  al  de  un  modelo  GPT2  pequeño,  específicamente  a  la  versión  más  pequeña  con  124  millones  de  parámetros,  como  se  describe  en  el  artículo  de  Radford  et  al. ,  \"Los  modelos  de  lenguaje  son  aprendices  multitarea  no  supervisados\".  Cabe  destacar  que,  si  bien  el  informe  original  menciona  117  millones  de  parámetros,  esto  se  corrigió  posteriormente.\n",
    "\n",
    "En  el  contexto  del  aprendizaje  profundo  y  LLM  como  GPT,  el  término  \"parámetros\"  se  refiere  a  los  pesos  entrenables  del  modelo.  Estos  pesos  son,  en  esencia,  las  variables  internas  del  modelo  que  se  ajustan  y  optimizan  durante  el  proceso  de  entrenamiento  para  minimizar  una  función  de  pérdida  específica.  Esta  optimización  permite  que  el  modelo  aprenda  de  los  datos  de  entrenamiento.\n",
    "\n",
    "Por  ejemplo,  en  una  capa  de  red  neuronal  representada  por  una  matriz  (o  tensor)  de  pesos  de  2048  x  2048  dimensiones,  cada  elemento  de  esta  matriz  es  un  parámetro.  Dado  que  hay  2048  filas  y  2048  columnas,  el  número  total  de  parámetros  en  esta  capa  es  2048  multiplicado  por  2048,  lo  que  equivale  a  4 194 304  parámetros.\n",
    "\n",
    "Ejemplo de configuración del pequeño modelo GPT-2 a traves del siguiente diccionario de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0374d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,      # Context length: Número max de tokens de entrada que el modelo puede manejar\n",
    "    \"emb_dim\": 768,       # Embedding dimension: tamaño de incrustación\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate: caida del 10%\n",
    "    \"qkv_bias\": False     # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7fab80",
   "metadata": {},
   "source": [
    "Utilizando  la  configuración  anterior,  comenzaremos  este  capítulo  implementando  una  arquitectura  de  marcador  de  posición  GPT  (DummyGPTModel)  en  esta  sección.  Esto  nos  proporcionará  na  visión  general  de  cómo  todo  encaja  y  qué  otros  componentes  necesitamos  codificar  en  las  siguientes  secciones  para  ensamblar  la  arquitectura  completa  del  modelo  GPT.\n",
    "\n",
    "![Texto alternativo](./imgs/4.3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estructura GPT provisional DummyGPTModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) #marcador de posición para TransformerBlock\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])          #marcador de posición para LayerNorm\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "class DummyTransformerBlock(nn.Module):                           #clase de marcador de posición simple que será reemplazado en un futuro\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):                                         #no se hace nada\n",
    "        return x\n",
    "class DummyLayerNorm(nn.Module):                                  #clase de marcador de posición simple que será reemplazado en un futuro\n",
    "    def __init__(self, normalized_shape, eps=1e-5):               #los aprámetros son solo para imitar la interfaz de LayerNorm\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
