{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ede9e",
   "metadata": {},
   "source": [
    "# Conexión de capas de atención y capas lineales en un bloque transformador\n",
    "\n",
    "En  esta  sección,  se implementará  el  bloque  transformador,  un  componente  fundamental  de  GPT  y  otras  arquitecturas  LLM.  Este  bloque,  que  se  repite  una  docena  de  veces  en  la  arquitectura  GPT2  de  124  millones  de  parámetros,  combina  varios  conceptos  que  ya  hemos  abordado:  atención  multicabezal,  normalización  de  capas,  abandono,  capas  de  avance  y  activaciones  GELU.\n",
    "\n",
    "![Texto alternativo](./imgs/4.13.png)\n",
    "\n",
    "El  bloque  transformador  combina  varios  componentes,  incluido  el  módulo  de  atención  de  múltiples  cabezas  enmascaradas  y  el  módulo  FeedForward.\n",
    "\n",
    "Cuando  un  bloque  transformador  procesa  una  secuencia  de  entrada,  cada  elemento  de  la  secuencia  (por  ejemplo,  un  token  de  palabra  o  subpalabra)  se  representa  mediante  un  vector  de  tamaño  fijo  (768  dimensiones).  Las  operaciones  dentro  del  bloque  transformador,  incluyendo  las  capas  de  atención  multicabezal  y  de  avance,  están  diseñadas  para  transformar  estos  vectores  de  forma  que  se  preserve  su  dimensionalidad.\n",
    "\n",
    "La  idea  es  que  el  mecanismo  de  autoatención  en  el  bloque  de  atención  de  múltiples  cabezas  identifique y  analiza  las  relaciones  entre  los  elementos  de  la  secuencia  de  entrada.  En  cambio,  la  red  de  propagación  hacia  adelante  modifica  los  datos  individualmente  en  cada  posición.  Esta  combinación  no  solo  permite  una  comprensión  y  un  procesamiento  más  precisos  de  la  entrada,  sino  que  también  mejora  la  capacidad  general  del  modelo  para  gestionar  patrones  de  datos  complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b007ad8",
   "metadata": {},
   "source": [
    "#### Paso 1: Convertir las palabras en perfiles numéricos (Vectores)\n",
    "\n",
    "Primero, la máquina no entiende de palabras como \"gato\" o \"casa\". Necesita convertirlas en algo que pueda procesar: números.\n",
    "\n",
    "* Cada palabra (o trozo de palabra, llamado **token**) se convierte en un **vector**: una larga lista de números (en tu ejemplo, 768 números).\n",
    "* Piensa en este vector como un **perfil o ficha de identidad súper detallado** de la palabra. Esta ficha no solo dice qué palabra es, sino que también captura su significado, su función gramatical y sus posibles contextos. Por ejemplo, los vectores de \"rey\" y \"reina\" serían muy parecidos entre sí.\n",
    "\n",
    "> **Clave:** Todas las \"fichas\" (vectores) tienen el mismo tamaño (768 dimensiones), lo que permite que la máquina las procese de manera uniforme.\n",
    "\n",
    "---\n",
    "\n",
    "#### Paso 2: El trabajo dentro del bloque (Los dos especialistas)\n",
    "\n",
    "Una vez que todas las palabras de la frase son \"fichas numéricas\", entran en el bloque transformador. Dentro, hay dos especialistas que trabajan en equipo:\n",
    "\n",
    "##### 1. La Capa de Atención Multicabezal: El Analista de Contexto \n",
    "\n",
    "Este es el especialista en **relaciones**. Su trabajo es mirar todas las palabras de la frase a la vez y averiguar cómo se conectan entre sí.\n",
    "\n",
    "* **Ejemplo:** En la frase \"El robot cogió la manzana porque estaba madura\", la palabra \"madura\" se refiere a la \"manzana\", no al \"robot\".\n",
    "* El mecanismo de **atención** es como un detective que traza líneas de conexión, dándole más importancia a la relación *\"manzana\"* <-> *\"madura\"* y muy poca a *\"robot\"* <-> *\"madura\"*.\n",
    "* Lo de **\"multicabezal\"** (*multi-head*) significa que no hay un solo detective, sino varios. Cada uno se especializa en buscar un tipo de relación diferente (uno busca sujeto-verbo, otro adjetivo-sustantivo, etc.).\n",
    "\n",
    "Al final de su trabajo, este especialista actualiza la \"ficha\" (el vector) de cada palabra, añadiéndole toda esta nueva información sobre su contexto y sus relaciones importantes en la frase.\n",
    "\n",
    "##### 2. La Red de Propagación Hacia Adelante: El Procesador Individual \n",
    "\n",
    "Después de que el primer especialista ha añadido el contexto, entra en juego el segundo. Este es un especialista que trabaja **individualmente** con cada palabra.\n",
    "\n",
    "* Toma la \"ficha\" ya enriquecida con el contexto y la procesa por su cuenta.\n",
    "* Su función es \"digerir\" esa nueva información y refinar el significado de esa palabra específica. Es como si, después de una reunión de equipo (la atención), cada miembro fuera a su escritorio a pensar profundamente sobre la información recibida y a hacer su propio trabajo.\n",
    "\n",
    "Este paso añade más capacidad de aprendizaje y permite al modelo captar conceptos más complejos y abstractos sobre cada elemento.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Por qué esta combinación es tan potente?\n",
    "\n",
    "La magia está en combinar estos dos pasos:\n",
    "\n",
    "1.  Primero, la **atención** mira el cuadro completo y entiende las **relaciones entre las piezas** (el contexto).\n",
    "2.  Luego, la **propagación hacia adelante** se enfoca en **cada pieza individualmente** para procesar y refinar su significado con ese nuevo contexto.\n",
    "\n",
    "Esta combinación permite que el modelo entienda no solo lo que significa cada palabra por sí sola, sino, y más importante, lo que significa dentro del tejido complejo de una frase. Al repetir este proceso en muchos bloques apilados, el modelo consigue una comprensión del lenguaje increíblemente profunda y matizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc05ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiHeadAttention import MultiHeadAttention\n",
    "from feedForward import FeedForward\n",
    "from layerNorm import LayerNorm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  \n",
    "    \"context_length\": 1024,     \n",
    "    \"emb_dim\": 768,     \n",
    "    \"n_heads\": 12,        \n",
    "    \"n_layers\": 12,   \n",
    "    \"drop_rate\": 0.1,    \n",
    "    \"qkv_bias\": False   \n",
    "}\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg=GPT_CONFIG_124M):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):   \n",
    "        ## Bloque 1 atencion\n",
    "        shortcut = x    #conexion de acceso directo para el bloque de atencion\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "\n",
    "        #Bloque 2: Red neuronal feed_forward\n",
    "        shortcut = x    #conexion de acceso directo para el bloque de avence\n",
    "        x = self.norm2(x)   \n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut    #agregaci´on de entrada principal\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7da13",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Diagrama del Flujo de Datos\n",
    "\n",
    "```text\n",
    "Entrada (x)\n",
    "   ↓\n",
    "┌──────────────────────────────┐\n",
    "│   Sub-bloque 1: Atención     │\n",
    "│------------------------------│\n",
    "│ 1. shortcut = x              │\n",
    "│ 2. norm1(x)                  │\n",
    "│ 3. att(x)                    │\n",
    "│ 4. drop_shortcut(x)          │\n",
    "│ 5. x = x + shortcut          │\n",
    "└──────────────────────────────┘\n",
    "   ↓\n",
    "┌──────────────────────────────┐\n",
    "│ Sub-bloque 2: Feed-Forward   │\n",
    "│------------------------------│\n",
    "│ 1. shortcut = x              │\n",
    "│ 2. norm2(x)                  │\n",
    "│ 3. ff(x)                     │\n",
    "│ 4. drop_shortcut(x)          │\n",
    "│ 5. x = x + shortcut          │\n",
    "└──────────────────────────────┘\n",
    "   ↓\n",
    "Salida\n",
    "```\n",
    "\n",
    "### Explicación de las Partes Clave\n",
    "1. Los Dos \"Especialistas\" Principales\n",
    "self.att = MultiHeadAttention(...): Este es el analista de contexto. Su única misión es mirar todos los vectores de la secuencia y recalcular cada uno de ellos para que contenga información sobre las palabras relevantes a su alrededor. Es el que decide que \"la\" en \"la gata\" se refiere a \"gata\".\n",
    "\n",
    "   self.ff = FeedForward(...): Este es el procesador individual. Una vez que la capa de atención ha enriquecido cada vector con su contexto, esta red neuronal simple procesa cada vector de forma aislada para extraer patrones más complejos y refinar su significado.\n",
    "\n",
    "2. Los Componentes de Soporte (¡Igual de importantes!)\n",
    "Normalización de Capa (self.norm1 y self.norm2): Piensa en esto como un \"control de volumen\". A medida que los datos pasan por muchas capas, los números de los vectores pueden volverse muy grandes o muy pequeños, lo que desestabiliza el aprendizaje. La normalización los mantiene en un rango \"saludable\", haciendo que el entrenamiento sea mucho más estable y rápido. Tu código usa una arquitectura Pre-LN (Pre-LayerNorm), donde la normalización se aplica antes de la atención y la red neuronal, lo cual es muy común y efectivo.\n",
    "\n",
    "   Conexiones Residuales (x = x + shortcut): Esta es una de las ideas más importantes. Al sumar la entrada original (shortcut) a la salida procesada, te aseguras de que el bloque no \"olvide\" la información original. Permite que la información fluya directamente a través del bloque sin ser modificada. Esto es crucial para poder entrenar modelos con muchísimas capas (cientos de bloques), ya que combate el problema de la \"desaparición del gradiente\". Es como una autopista que permite a la información original saltarse el procesamiento si es necesario.\n",
    "\n",
    "   Dropout (self.drop_shortcut): Esta es una técnica para evitar que el modelo \"memorice\" los datos de entrenamiento en lugar de aprender patrones generales. Durante el entrenamiento, apaga aleatoriamente algunas neuronas. Esto obliga al resto de la red a aprender de una forma más robusta y a no depender de unas pocas neuronas específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2b53faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #[batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb969dd6",
   "metadata": {},
   "source": [
    "![Texto alternativo](./imgs/4.14.png)\n",
    "\n",
    "El  bloque  transformador  combina  la  normalización  de  capas,  la  red  de  alimentación  hacia  adelante  (incluidas  las  activaciones  GELU)  y  las  conexiones  de  acceso  directo. Este  bloque  transformador  constituirá  el  componente  principal  de  la  arquitectura  GPT  que se implementará.\n",
    "\n",
    "[Codificación del modelo GPT](./6_codificacion_modelo_gpt.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
