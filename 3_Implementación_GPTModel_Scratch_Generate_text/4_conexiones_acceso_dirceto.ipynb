{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe22d4e",
   "metadata": {},
   "source": [
    "# Agregar conexiones de acceso directo\n",
    "\n",
    "Originalmente,  las  conexiones  de  acceso  directo  se  propusieron  para  redes  profundas  en  visión  artificial  (específicamente,  en  redes  residuales)  para  mitigar  el  desafío  de  los  gradientes  evanescentes.\n",
    "El  problema  del  gradiente  que  desaparece  se  refiere  al  problema  en  el  cual  los  gradientes  (que  guían  las  actualizaciones  de  peso  durante  el  entrenamiento)  se  vuelven  progresivamente  más  pequeños  a  medida  que  se  propagan  hacia  atrás  a  través  de  las  capas,  lo  que  dificulta  el  entrenamiento  efectivo  de  las  capas  anteriores.\n",
    "\n",
    "![Texto alternativo](./imgs/4.12.png)\n",
    "\n",
    "Comparación  entre  una  red  neuronal  profunda  de  5  capas  sin  conexiones  de  acceso  directo  (izquierda)  y  con  conexiones  de  acceso  directo  (derecha).  Las  conexiones  de  acceso  directo  implican  sumar  las  entradas  de  una  capa  a  sus  salidas,  creando  así  una  ruta  alternativa  que  omite  ciertas  capas.\n",
    "\n",
    "Una  conexión  de  acceso  directo  crea  una  ruta  alternativa  más  corta  para  que  el  gradiente  fluya  a  través  de  la  red  saltando  una  o  más  capas.  Esto  se  logra  sumando  la  salida  de  una  capa  a  la  de  una  capa  posterior.  \n",
    "Por  eso,  estas  conexiones  también  se  conocen  como  conexiones  de  salto.  Desempeñan  un  papel  crucial  en  la  preservación  del  flujo  de  gradientes  durante  el  paso  hacia  atrás  en  el  entrenamiento.\n",
    "````\n",
    "x ───────────────┐\n",
    "                 ▼\n",
    "              [Bloque]\n",
    "                 ▼\n",
    "             salida = Bloque(x) + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f69d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            #Implementación de 5 capas\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            #salida de la capa actual\n",
    "            layer_output = layer(x)\n",
    "            #mirar si se puede ahcer shortcut\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80d7e",
   "metadata": {},
   "source": [
    "El  código  implementa  una  red  neuronal  profunda  con  cinco  capas,  cada  una  compuesta  por  una  capa  lineal  y  una  función  de  activación  GELU .  En  el  paso  directo,  pasamos  iterativamente  la  entrada  a  través  de  las  capas  y,  opcionalmente,  añadimos  las  conexiones  de  acceso  directo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305f9c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "#Red neuronal sin conexiones de accesos directo\n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcup = ExampleDeepNeuralNetwork(layer_sizes=layer_sizes, use_shortcut=False)\n",
    "\n",
    "#Función que calcula los gradientes en la regresión del modelo\n",
    "def print_gradients(model, x):\n",
    "    #Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    #Calcular la perdida de como de cerca del target la salida está\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    #Backward pass para calcular gradientes\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "print_gradients(model_without_shortcup, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af937a7",
   "metadata": {},
   "source": [
    "En  el  código  anterior,  se especifica  una  función  de  pérdida  que  calcula  la  proximidad  entre  la  salida  del  modelo  y  un  objetivo  especificado  por  el  usuario  (aquí,  para  simplificar,  el  valor  0).  Luego,  al  llamar  a  loss.backward(),  PyTorch  calcula  el  gradiente  de  pérdida  para  cada  capa  del  modelo.  Podemos  iterar  \n",
    "sobre  los  parámetros  de  peso  mediante  model.named_parameters().  Supongamos  que  tenemos  una  matriz  de  parámetros  de  peso  de  3×3  para  una  capa  dada.  En  ese  caso,  esta  capa  tendrá  valores  de  gradiente  de  3×3,  e  imprimimos  el  gradiente  absoluto  medio  de  estos  valores  de  gradiente  de  3×3  para  \n",
    "obtener  un  único  valor  de  gradiente  por  capa  y  comparar  los  gradientes  entre  capas  con  mayor  facilidad.\n",
    "En  resumen,  el  método .backward()  es  un  método  práctico  en  PyTorch  que  calcula  los  gradientes  de  pérdida,  necesarios  durante  el  entrenamiento  del  modelo,  sin  tener  que  implementar  los  cálculos  matemáticos  para  el  gradiente,  lo  que  facilita  enormemente  el  trabajo  con  redes  neuronales  profundas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f03ab3",
   "metadata": {},
   "source": [
    "Los  gradientes  se  vuelven  más  pequeños  a  medida  que  avanzamos  desde  la  última  capa  (capas.4)  a  la  primera  capa  (capas.0),  lo  que  es  un  fenómeno  llamado  el  problema  del  gradiente  que  desaparece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3d9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "#Red neuronal con conexiones directas\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "                        layer_sizes, use_shortcut=True\n",
    "                        )\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88e659",
   "metadata": {},
   "source": [
    "Como se puede ver,  según  el  resultado,  la  última  capa  (capas.4)  aún  tiene  un  gradiente  mayor  que  las  demás.  Sin  embargo,  el  valor  del  gradiente  se  estabiliza  a  medida  que  avanzamos  hacia  la  primera  capa  (capas.0)  y  no  se  reduce  a  un  valor  extremadamente  pequeño.\n",
    "\n",
    "En  conclusión,  las  conexiones  de  acceso  directo  son  importantes  para  superar  las  limitaciones  que  plantea  el  problema  del  gradiente  evanescente  en  redes  neuronales  profundas.  Las  conexiones  de  acceso  directo  son  un  componente  fundamental  de  modelos  muy  grandes,  como  los  LLM,  y  facilitarán  un  entrenamiento  más  eficaz  al  garantizar  un  flujo  de  gradiente  consistente  entre  capas  cuando  entrenemos  el  modelo  GPT  en  el  siguiente  capítulo.\n",
    "\n",
    "En la siguiente sección se conectarán todos los pasos hasta ahora descritos (normalización, GELU, modulo de avance y conexiones de acceso directo) en un bloque de transformador.\n",
    "\n",
    "[Conexión de capas de atención y capas lineales en un bloque transformador](./5_capas_atencion_lineales_transformador.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
