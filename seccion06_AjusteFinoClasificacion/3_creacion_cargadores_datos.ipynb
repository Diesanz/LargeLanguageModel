{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8c70d2",
   "metadata": {},
   "source": [
    "# Creación de cargadores de datos\n",
    "\n",
    "En esta sección se van a crear cargadores de datos de Pythorch similares a los de las seccion_02.\n",
    "\n",
    "Anteriormente se utilizó una técnica de ventana deslizante para generar fragmentos de texto de tamaño uniforme los cuales  luego  se  agruparon  en  lotes  para  un  entrenamiento  del  modelo  más  eficiente.\n",
    "Cada  fragmento  funcionó  como  una  instancia  de  entrenamiento  individual.\n",
    "\n",
    "Sin embargo, en esta sección, se va a trabajar  con  un  conjunto  de  datos  de  spam  que  contiene  mensajes  de  texto  de  distinta  longitud.  Para  agrupar  estos  mensajes, se tienen dos opciones:\n",
    "\n",
    "- Truncar todos los mensajes a la longitud del mensaje más corto del conjunto de datos o lote.\n",
    "- Rellena todos los mensajes hasta la longitud del mensaje más largo del conjunto de datos o lote.\n",
    "\n",
    "La  opción  1  es  computacionalmente  más  económica,  pero  puede  resultar  en  una  pérdida  significativa  de  información  si  los  mensajes  más  cortos  son  mucho  más  pequeños  que  los  mensajes  promedio  o  más  largos,  lo  que  podría  reducir  el  rendimiento  del  modelo. \n",
    "\n",
    "Para  implementar  la  opción  2,  donde  todos  los  mensajes  se  rellenan  hasta  la  longitud  del  mensaje  más  largo  del  conjunto  de  datos,  se añaden  tokens  de  relleno  a  todos  los  mensajes  más  cortos.  Para  ello,  se usa  \"<|endoftext|>\"  como  token  de  relleno.\n",
    "\n",
    "Sin  embargo,  en  lugar  de  agregar  la  cadena  \"<|endoftext|>\"  a  cada  uno  de  los  mensajes  de  texto  directamente, se puede  agregar  el  ID  de  token  correspondiente  a  \"<|endoftext|>\"  a  los  mensajes  de  texto  codificados.\n",
    "\n",
    "![Texto alternativo](./imgs/6.5.png)\n",
    "\n",
    "Suponer  que  50256  es  el  ID  del  token  de  relleno  \"<|endoftext|>\".\n",
    "Se puede verificar  que  este  sea  el  ID  de  token  correcto  codificando  \"  <|endoftext|>\"  usando  el  tokenizador  GPT2  del  paquete  tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df040473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d8b44",
   "metadata": {},
   "source": [
    "Primero es necesario implementar  un  conjunto  de  datos  de  PyTorch,  para especificar  cómo  se  cargan  y  procesan  los  datos,  antes  de  que  se puedan  instanciar  los  cargadores  de  datos.\n",
    "\n",
    "Para  ello,  se define  la  clase  SpamDataset ,  que  implementa  los  conceptos  ilustrados  en  la  figura anterior.  Esta  clase  SpamDataset  gestiona  varias  tareas  clave:  identifica  la  secuencia  más  larga  del  conjunto  de  datos  de  entrenamiento,  codifica  los  mensajes  de  texto  y  garantiza  que  todas  las  demás  secuencias  se  rellenen  con  un  token  de  relleno  para  que  coincida  con  la  longitud  de  la  secuencia  más  larga.secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "df_train = pd.read_csv(\"./CSV/train.csv\")\n",
    "encoded_text = [tokenizer.encode(text) for text in df_train[\"Text\"]]\n",
    "\n",
    "maxq = 0\n",
    "for text in encoded_text:\n",
    "    len_1 = len(text)\n",
    "    if len_1 > max:\n",
    "        maxq = len_1\n",
    "\n",
    "encoded_text = [text + [\"50256\"] * (maxq - len(text)) for text in encoded_text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
