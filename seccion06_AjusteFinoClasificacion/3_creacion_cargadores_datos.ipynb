{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8c70d2",
   "metadata": {},
   "source": [
    "# Creación de cargadores de datos\n",
    "\n",
    "En esta sección se van a crear cargadores de datos de Pythorch similares a los de las seccion_02.\n",
    "\n",
    "Anteriormente se utilizó una técnica de ventana deslizante para generar fragmentos de texto de tamaño uniforme los cuales  luego  se  agruparon  en  lotes  para  un  entrenamiento  del  modelo  más  eficiente.\n",
    "Cada  fragmento  funcionó  como  una  instancia  de  entrenamiento  individual.\n",
    "\n",
    "Sin embargo, en esta sección, se va a trabajar  con  un  conjunto  de  datos  de  spam  que  contiene  mensajes  de  texto  de  distinta  longitud.  Para  agrupar  estos  mensajes, se tienen dos opciones:\n",
    "\n",
    "- Truncar todos los mensajes a la longitud del mensaje más corto del conjunto de datos o lote.\n",
    "- Rellena todos los mensajes hasta la longitud del mensaje más largo del conjunto de datos o lote.\n",
    "\n",
    "La  opción  1  es  computacionalmente  más  económica,  pero  puede  resultar  en  una  pérdida  significativa  de  información  si  los  mensajes  más  cortos  son  mucho  más  pequeños  que  los  mensajes  promedio  o  más  largos,  lo  que  podría  reducir  el  rendimiento  del  modelo. \n",
    "\n",
    "Para  implementar  la  opción  2,  donde  todos  los  mensajes  se  rellenan  hasta  la  longitud  del  mensaje  más  largo  del  conjunto  de  datos,  se añaden  tokens  de  relleno  a  todos  los  mensajes  más  cortos.  Para  ello,  se usa  \"<|endoftext|>\"  como  token  de  relleno.\n",
    "\n",
    "Sin  embargo,  en  lugar  de  agregar  la  cadena  \"<|endoftext|>\"  a  cada  uno  de  los  mensajes  de  texto  directamente, se puede  agregar  el  ID  de  token  correspondiente  a  \"<|endoftext|>\"  a  los  mensajes  de  texto  codificados.\n",
    "\n",
    "![Texto alternativo](./imgs/6.5.png)\n",
    "\n",
    "Suponer  que  50256  es  el  ID  del  token  de  relleno  \"<|endoftext|>\".\n",
    "Se puede verificar  que  este  sea  el  ID  de  token  correcto  codificando  \"  <|endoftext|>\"  usando  el  tokenizador  GPT2  del  paquete  tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df040473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d8b44",
   "metadata": {},
   "source": [
    "Primero es necesario implementar  un  conjunto  de  datos  de  PyTorch,  para especificar  cómo  se  cargan  y  procesan  los  datos,  antes  de  que  se puedan  instanciar  los  cargadores  de  datos.\n",
    "\n",
    "Para  ello,  se define  la  clase  SpamDataset ,  que  implementa  los  conceptos  ilustrados  en  la  figura anterior.  Esta  clase  SpamDataset  gestiona  varias  tareas  clave:  identifica  la  secuencia  más  larga  del  conjunto  de  datos  de  entrenamiento,  codifica  los  mensajes  de  texto  y  garantiza  que  todas  las  demás  secuencias  se  rellenen  con  un  token  de  relleno  para  que  coincida  con  la  longitud  de  la  secuencia  más  larga.secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcec6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "df_train = pd.read_csv(\"./CSV/train.csv\")\n",
    "encoded_text = [tokenizer.encode(text) for text in df_train[\"Text\"]]\n",
    "\n",
    "maxq = 0\n",
    "for text in encoded_text:\n",
    "    len_1 = len(text)\n",
    "    if len_1 > maxq:\n",
    "        maxq = len_1\n",
    "\n",
    "encoded_text = [text + [\"50256\"] * (maxq - len(text)) for text in encoded_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2aa782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]] #pre-tokenizar el texto\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()    #truncar secuencias si son más largas que max_lenght\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "                                                                \n",
    "            self.encoded_texts = [  \n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        self.encoded_texts = [text + [pad_token_id] * (maxq - len(text)) for text in self.encoded_texts]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.loc[index][\"Label\"]\n",
    "        return (torch.tensor(encoded), torch.tensor(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        maxq = 0\n",
    "        for text in self.encoded_texts:\n",
    "            len_1 = len(text)\n",
    "            if len_1 > maxq:\n",
    "                maxq = len_1\n",
    "        return maxq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aecab5",
   "metadata": {},
   "source": [
    "La  clase  SpamDataset  carga  datos  de  los  archivos  CSV,  tokeniza  el  texto  mediante  el  tokenizador  GPT2  de  tiktoken  y  permite  rellenar  o  truncar  las  secuencias  a  una  longitud  uniforme,  determinada  por  la  secuencia  más  larga  o  una  longitud  máxima  predefinida.  Esto  garantiza  que  cada  tensor  de  entrada  tenga  el  mismo  tamaño,  lo  cual  es  necesario  para  crear  los  lotes  en  el  cargador  de  datos  de  entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbd474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"CSV/train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7ee4a",
   "metadata": {},
   "source": [
    "A taner en cuenta que  la  longitud  de  secuencia  más  larga  se  almacena  en  el  atributo  max_length  del  conjunto  de  datos ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a13a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74904813",
   "metadata": {},
   "source": [
    "El  código  genera  120,  lo  que  indica  que  la  secuencia  más  larga  no  contiene  más  de  120  tokens,  una  longitud  común  para  mensajes  de  texto.  Cabe  destacar  que  el  modelo  admite  secuencias  de  hasta  1024  tokens,  dado  el  límite  de  longitud  de  contexto.  Si  su  conjunto  de  datos  incluye  textos  más  largos,  puede  pasar  max_length=1024  al  crear  el  conjunto  de  datos  de  entrenamiento  en  el  código  anterior  para  garantizar  que  los  datos  no  excedan  la  longitud  de  entrada  (contexto)  admitida  por  el  modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da127b61",
   "metadata": {},
   "source": [
    "A continuación, se rellena los conjuntos de validación y prueba para que coincidan con la longitud de la secuencia de entrenamiento más larga. Es importante tener en cuenta que cualquier muestra del conjunto de validación y prueba que exceda la longitud máxima de entrenamiento se trunca mediante encoded_text[:self.max_length] en el código de SpamDataset que que definio anteriormente.\n",
    "\n",
    "El truncamiento es opcional; también puede establecer max_length=None tanto para los conjuntos de validación como de prueba, siempre que no haya secuencias que excedan los 1.024 tokens en estos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05314e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"CSV/validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"CSV/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295155b",
   "metadata": {},
   "source": [
    "Usando  los  conjuntos  de  datos  como  entradas,  se pueden  instanciar  los  cargadores  de  datos.  Sin  embargo,  en  este  caso,  los  objetivos  representan  etiquetas  de  clase  en  lugar  de  la  siguiente tokens  en  el  texto.  Por  ejemplo,  al  elegir  un  tamaño  de  lote  de  8,  cada  lote  constará  de  ejemplos  de  entrenamiento  de  longitud  120  y  la  etiqueta  de  clase  correspondiente  de  cada  ejemplo.\n",
    "\n",
    "![Texto alternativo](./imgs/6.6.png)\n",
    "\n",
    "El  siguiente  código  se crea  los  cargadores  de  datos  del  conjunto  de  entrenamiento,  validación  y  prueba  que  cargan  los  mensajes  de  texto  y  las  etiquetas  en  lotes  de  tamaño  8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7731ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa2d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#Comprobación de que los cargadores están funcionando\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e060be",
   "metadata": {},
   "source": [
    "Como se puede ver, los lotes de entrada constan de 8 ejemplos de entrenamiento con  120  tokens  cada  uno,  como  se  esperaba.  El  tensor  de  etiquetas  almacena  las  etiquetas  de  clase  correspondientes  a  los  8  ejemplos  de  entrenamiento. Por  último,  para  tener  una  idea  del  tamaño  del  conjunto  de  datos,  se imprimen el  número  total  de  lotes  en  cada  conjunto  de  datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6067dc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fd2ec",
   "metadata": {},
   "source": [
    "Con  esto  concluye  la  preparación  de  datos  de  este  capítulo.  A  continuación, se preparará el  modelo  para  el  ajuste  fino.\n",
    "\n",
    "[Inicialización de un modelo con pesos preentrenados](./4_inicializacion_modelo_pesos_preentrenados.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
