{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58b77a8",
   "metadata": {},
   "source": [
    "# Creación de cargadores de datos para un conjunto de datos de instrucciones\n",
    "\n",
    "En la sección anterior, se introdujo una clase InstructionDataset y una función custom_collate_fn para el conjunto de datos de instrucciones.\n",
    "\n",
    "Aprovechando el trabajo anterior se pueden conectar los objetos InstructionDataset y la función custom_collate_fn a los cargadores de datos de Pytorch. Estos  cargadores  reorganizarán  y  organizarán  automáticamente  los  lotes  para  el  proceso  de  ajuste  fino  de  las  instrucciones  LLM.\n",
    "\n",
    "![Texto alternativo](./imgs/7.14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223b86e",
   "metadata": {},
   "source": [
    "La  función  custom_collate_fn  incluye  código  para  mover  los  tensores  de  entrada  y  de  destino  (por  ejemplo,  torch.stack(inputs_lst).to(device))  a  un  dispositivo  específico,  que  puede  ser  \"cpu\"  o  \"cuda\"  (para  GPU),  u  opcionalmente  \"mps\"  para  Mac  con  chips  Apple  Silicon.(A tener  en  cuenta  que  el  uso  de  un  dispositivo  \"mps\"  puede  generar  diferencias  numéricas  en  comparación  con  el  contenido  de  este  capítulo,  ya  que  la  compatibilidad  con  Apple  Silicon  en  PyTorch  aún  es  experimental).\n",
    "\n",
    "En secciones anteriores, se movió  los  datos  al  dispositivo  de  destino  (por  ejemplo,  la  memoria  de  la  GPU  cuando  device=\"cuda\")  en  el  bucle  de  entrenamiento  principal.  Incorporar  esto  como  parte  de  la  función  de  intercalación  ofrece  la  ventaja  de  realizar  este  proceso  de  transferencia  de  dispositivo  en  segundo  plano,  fuera  del  bucle  de  entrenamiento,  lo  que  evita  que  bloquee  la  GPU  durante  el  entrenamiento  del  modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57328f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")                           \n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08436517",
   "metadata": {},
   "source": [
    "A  continuación,  para  reutilizar  la  configuración  del  dispositivo  seleccionada  en  custom_collate_fn  al  conectarla  a  la  clase  DataLoader  de  PyTorch  más  adelante  en  esta  sección,  usando  la  función  parcial  de  la  biblioteca  estándar  functools  de  Python  para  crear  una  nueva  versión  de  la  función  con  el  argumento  del  dispositivo  predefinido.  Además, se establece  el  valor  de  allowed_max_length  en  1024,  lo  que  trunca  los  datos  a  la  longitud  máxima  de  contexto  compatible  con  el  modelo  GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f122070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_collocta import custom_collate_fn\n",
    "from functools import partial\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e6f42",
   "metadata": {},
   "source": [
    "A continuación, se puede configurar los cargadores de datos como se hizo anteriormente, pero esta vez se usará la función de intercalación personalizada para el proceso de procesamiento de lotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9416581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from instructionDataset import InstructionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "with open('train.json', \"r\", encoding=\"utf-8\") as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open('val.json', \"r\", encoding=\"utf-8\") as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "with open('test.json', \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = json.load(file)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39296d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 55]) torch.Size([8, 55])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 56]) torch.Size([8, 56])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587adcff",
   "metadata": {},
   "source": [
    "En  el  resultado  anterior, se puede ver  que  el  primer  lote  de  entrada  y  el  lote  de  destino  tienen  dimensiones  de  8×61,  donde  8  representa  el  tamaño  del  lote  y  61  el  número  de  tokens  en  cada  ejemplo  de  entrenamiento  de  este  lote.  El  segundo  lote  de  entrada  y  el  lote  de  destino  tienen  un  número  de  tokens  diferente;  por  ejemplo,  76.\n",
    "\n",
    "Como  se vió  en  el  código  anterior,  gracias  a  la función  de  intercalación  personalizada,  el  cargador  de  datos  puede  crear  lotes  de  diferentes  longitudes.  En  la siguiente  sección, se cargará  un  LLM  preentrenado  que se puede ajustar  con  este  cargador  de  datos.\n",
    "\n",
    "\n",
    "[Carga de un LLM preentrenado](./5_carga_llm_preentrenado.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
