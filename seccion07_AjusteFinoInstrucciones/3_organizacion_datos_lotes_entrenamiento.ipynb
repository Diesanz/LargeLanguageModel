{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e841d3",
   "metadata": {},
   "source": [
    "#  Organización de datos en lotes de entrenamiento\n",
    "\n",
    "El siguiente paso es construir los lotes de entrenamiento de manera efectiva. Esto implica definir un método que garantice que el modelo reciba los datos de entrenamiento formateados durante el proceso de ajuste.\n",
    "\n",
    "![Texto alternativo](./imgs/7.5.png)\n",
    "\n",
    "En la sección 06,   los  lotes  de  entrenamiento  fueron  creados  automáticamente  por  la  clase  DataLoader  de  PyTorch ,  que  emplea  una  función  de  intercalación  predeterminada  para  combinar  listas  de  muestras  en  lotes.  Esta  función  se  encarga  de  tomar  una  lista  de muestras  de  datos  individuales  y  combinarlas  en  un  único  lote  que  el  modelo  puede  procesar  eficientemente  durante  el  entrenamiento.\n",
    "\n",
    "Sin  embargo,  el  proceso  de  procesamiento  por  lotes  para  el  ajuste  fino  de  instrucciones  en  esta sección es  un  poco  más  complejo  y  requiere  la  creación  de  una  función  de intercalación  personalizada  que  posteriormente se conectará  al  DataLoader.\n",
    "\n",
    "En  esta  sección,  se abordará  el  proceso  de  procesamiento  por  lotes  en  varios  pasos,  incluida  la  codificación de  la  función  de  intercalación  personalizada.\n",
    "\n",
    "![Texto alternativo](./imgs/7.6.png)\n",
    "\n",
    "Primero se implementan los pasos 2.1 y 2.2, se codifica una clase InstructionDataset que aplica format_input y pre_tokeniza todas las entradas en el conjunto de datos.\n",
    "\n",
    "![Texto alternativo](./imgs/7.7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee51f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95aeebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_text = []\n",
    "\n",
    "        for entry in data:                                       \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_text[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d27627",
   "metadata": {},
   "source": [
    "Similar al enfoque de la sección 06, se busca acelerar el entrenamiento recopilando múltiples ejemplos de entrenamiento en un lote, lo que requiere rellenar todas las entradas con una logitud similar.  En  lugar  de  añadir  los  tokens  <|endoftext|>  a  las  entradas  de  texto,  se puede añadir  su  ID  de  token  directamente  a  las  entradas  pretokenizadas.  \n",
    "\n",
    "En  la sección 06, se igualaba  la  longitud  de  todos  los  ejemplos  de  un  conjunto  de  datos.  Pasando  al  paso  2.3  de  la  figura, se adopta  un  enfoque  más  sofisticado:  desarrollar  una  función  de  intercalación  personalizada  que  se puede pasar  al  cargador  de  datos.  Esta  función  de  intercalación  personalizada  iguala  la  longitud  de  los  ejemplos  de  entrenamiento  de  cada  lote,  permitiendo  que  los  distintos  lotes  tengan  longitudes  diferentes.  Este  enfoque  minimiza  el  relleno  innecesario,  extendiendo  las  secuencias  solo  para  que  coincidan  con  la  más  larga  de  cada  lote,  no  con  todo  el  conjunto  de  datos.\n",
    "\n",
    "![Texto alternativo](./imgs/7.8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53ea6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_1(batch, pad_token_id=50256, device='cpu'):\n",
    "    #Calculamos las longitudes originales de cada secuencia\n",
    "    batchs_lengths = [len(item) for item in batch]\n",
    "    #Determinamos la longitud máxima del batch (+1 para añadir el token final)\n",
    "    batch_max_length = max(batchs_lengths) + 1\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        item += [pad_token_id]\n",
    "        padded = item + [pad_token_id] * (batch_max_length - len(item))\n",
    "        #Creamos el tensor de entrada quitando el último token ([:-1])\n",
    "        #Esto se hace para alinear inputs y labels en el entrenamiento autoregresivo\n",
    "        #El modelo ve \"inputs\" y debe predecir el siguiente token (label)\n",
    "        #Por eso inputs = secuencia sin el último token\n",
    "        #y labels  = secuencia sin el primero\n",
    "        input = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(input)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90149496",
   "metadata": {},
   "source": [
    "Como se puede  ver  en  base  al  resultado  anterior,  todas  las  entradas  se  han  rellenado  hasta  la  longitud  dela  lista  de  entrada  más  larga,  inputs_1,  que  contiene  5  ID  de  token. Sin  embargo,  como  se vio en las secciones 05  y  06,  también  se necesitan  crear  lotes con  los  ID  de  token  de  destino,  correspondientes  al  lote  de  ID  de  entrada.  Estos  ID  de  destino, son  cruciales  porque  representan  lo  que quiere  que  el  modelo  genera y  lo  que  se necesita  durante  el  entrenamiento  para  calcular  la  pérdida  para  las  actualizaciones  de  peso.\n",
    "\n",
    "![Texto alternativo](./imgs/7.9.png)\n",
    "\n",
    "\n",
    "Similar  al  proceso  descrito  en la sección 05  para  el  preentrenamiento  de  un  LLM,  los  ID  de  los  tokens  de  destino  coinciden  con  los  de  entrada,  pero  se  desplazan  una  posición  a  la  derecha.  Esta  configuración,  como  se  muestra  en  la  figura,  permite  al  LLM  aprender  a  predecir  el  siguiente  token  en  una  secuencia.\n",
    "\n",
    "![Texto alternativo](./imgs/7.10.png)\n",
    "\n",
    "La  siguiente  función  de  intercalación  actualizada  genera  los  ID  de  token  de  destino,  a  partir  de  los  ID  de  token  de  entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae78dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_2(batch,pad_token_id=50256,device=\"cpu\"):\n",
    "    batchs_lengths = [len(item) for item in batch]\n",
    "    batch_max_length = max(batchs_lengths) + 1\n",
    "    print(batch_max_length)\n",
    "    inputs_lst, target_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        item += [pad_token_id]\n",
    "        padded = item + [pad_token_id] * (batch_max_length - len(item))\n",
    "        input = torch.tensor(padded[:-1]) #tensor de entradas\n",
    "        target = torch.tensor(padded[1:]) #tensor de salidas\n",
    "        inputs_lst.append(input)\n",
    "        target_lst.append(target)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    target_tensor = torch.stack(target_lst).to(device)\n",
    "    return inputs_tensor, target_tensor\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01153f6b",
   "metadata": {},
   "source": [
    "En  el  siguiente  paso,  se asigna un  valor  de  marcador  de  posición  -100  a  todos  los  tokens  de  relleno. Este  valor  especial   permite  excluir  estos  tokens  de  relleno  de  contribuir  al  cálculo  de  la  pérdida  de  entrenamiento,  garantizando  que  solo  los  datos  significativos  influyan  en  el  aprendizaje  del  modelo.\n",
    "\n",
    "En la sección 06, no se tuvo que tener en detalle esto, ya que solo se entrenaba el modelo con el último token de salida.\n",
    "\n",
    "![Texto alternativo](./imgs/7.11.png)\n",
    "\n",
    "En  el  paso  2.4,  como  se  muestra  en  la  figura, se reemplaza  los  tokens  de  fin  de  texto,  que  previamente se usó  como  tokens  de  relleno  y  cuyo  ID  de  token  es  50256,  por  100  en  la  lista  de  tokens  de  destino.  (La  elección  de  -100  como  reemplazo  se  aclarará  más  adelante).\n",
    "\n",
    "Sin  embargo,  a tener  en  cuenta  que se conserva  un  token  de  fin  de  texto,  ID  50256,  en  la  lista  de  destinos. Esto  permite  que  el  LLM  aprenda  cuándo  generar  un  token  de  fin  de  texto  en  respuesta  a  las  instrucciones,  lo  cual  se utiliza  como  indicador  de  que  la  respuesta  generada  está  completa.\n",
    "\n",
    "![Texto alternativo](./imgs/7.12.png)\n",
    "\n",
    "En  el  siguiente  código,  se modifica  la  función  de  intercalación  personalizada  para  reemplazar  los  tokens  con  ID  50256  por  -100  en  las  listas  de  destino.  Además, se introduce  el  parámetro  \"allowed_max_length \"  para  limitar  opcionalmente  la  longitud  de  las  muestras.  Este  ajuste  será  útil  si  se planea  trabajar  con  conjuntos  de  datos  propios  que  superen  el  tamaño  de  contexto  de  1024  tokens  compatible  con  el  modelo  GPT2.  El  código  para  esta  función  de  intercalación  actualizada  es  el  siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93d7901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batchs_lengths = [len(item) for item in batch]\n",
    "    batch_max_length = max(batchs_lengths) + 1\n",
    "    \n",
    "    inputs_lst, target_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()  #copiar para evitar pisar el batch original\n",
    "        new_item += [pad_token_id]\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1]) #tensor de entradas (truncado)\n",
    "        targets = torch.tensor(padded[1:]) #tensor de salidas (shift +1)\n",
    "\n",
    "        mask = targets == pad_token_id  #Reemplazar  todos  los  tokens  de  relleno  excepto  el  primero  en  los  objetivos  por  ignore_index\n",
    "        indices = torch.nonzero(mask).squeeze()  #devuelve las posiciones (índices) donde mask es True y lo aplana un poco\n",
    "        if indices.numel() > 1: #si el número de elementos de un tensor es mayor que 1\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]                  #Truncar  opcionalmente  a  la  longitud  máxima  de  secuencia\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        target_lst.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    target_tensor = torch.stack(target_lst).to(device)\n",
    "    return inputs_tensor, target_tensor\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b302df",
   "metadata": {},
   "source": [
    "La  función  de  intercalación  modificada  funciona  como  se  espera,  modificando  la  lista  de  destino  insertando  el ID  de  token  -100:\n",
    "\n",
    "**El proposito de esta modificación es:**\n",
    "\n",
    "Para  fines  de  demostración,  se considera  el  siguiente  ejemplo  simple  y  autónomo, donde  cada  logit  de  salida  puede  corresponder  a  un  token  potencial  del  vocabulario  del  modelo. Así  es  como se podría  calcular  la  pérdida  de  entropía  cruzada  (introducida  en  la sección 05)  durante el entrenamiento  cuando  el  modelo  predice  una secuencia  de  tokens,  similar  a  lo  que  se ha hecho  en la sección 05  cuando  se  entrena  previamente  el  modelo,  o  en  la sección  06  cuando  se  ajusta  el  modelo  para la clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89609c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0,1.0], #predicciones para el primer token\n",
    "    [-0.5, 1.5]] #predicciones para el segundo token\n",
    ")\n",
    "\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bd24e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "#Agregar un ID de token adicional, como es de esperar afectará añ cálculo de la pérdida\n",
    "\n",
    "logits_2 = torch.tensor(\n",
    "[[-1.0, 1.0],\n",
    "[-0.5, 1.5],\n",
    "[-0.5, 1.5]]                                                 #A\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74533088",
   "metadata": {},
   "source": [
    "Hasta  ahora,  se ha realizado  algunos  cálculos  de  ejemplo  más  o  menos  obvios  utilizando  la  función  de  pérdida  de  entropía  cruzada  en  PyTorch,  la  misma  función  de  pérdida  que  se ha usadao  en  las  funciones  de  entrenamiento  de  las secciones 05 y 06.\n",
    "\n",
    "Ahora, la  parte  interesante, ver  qué  sucede  si se reemplaza el  tercer  objetivo ID  de  token  con  -100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8222420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a35c9",
   "metadata": {},
   "source": [
    "Con  base  en  este  resultado,  se puede  observar  que  la  pérdida  resultante  en  estos  tres  ejemplos  de  entrenamiento  es  idéntica  a  la  que calculó  a  partir  de  los  dos  ejemplos  de  entrenamiento  anteriores.  En  otras  palabras,  la  función  de  pérdida  de  entropía  cruzada  ignoró  la  tercera  entrada  del  vector  targets_3 ,  cuyo  ID  de  token  corresponde  a  -100.\n",
    "\n",
    "Entonces,  **¿qué  tiene  de  especial  100  que  la  pérdida  de  entropía  cruzada  lo  ignora?**  La  configuración  predeterminada  de  la  función  de  entropía  cruzada  en  PyTorch  es  cross_entropy(...,  ignore_index=100).  Esto  significa  que  ignora  los  objetivos  etiquetados  con  -100.\n",
    "\n",
    "Se va a provechar este ignore_index para ignorar los tokens de relleno (“relleno” que solo está ahí para que todas las secuencias tengan la misma longitud) adicionales.\n",
    "\n",
    "Sin embargo, se quiere mantener un ID de token 50356 ya que ayuda al LLM a aprender a generar tokens de fin de texto, que se puede utilizar para indicar que una respuesta está completa.\n",
    "Además de enmascarar los tokens de relleno, también es común enmascarar el objetivo.\n",
    "\n",
    "![Texto alternativo](./imgs/7.13.png)\n",
    "\n",
    "Al  ocultar  los  identificadores  de  token  de  destino  correspondientes  a  la  instrucción,  como  se  muestra  en  la  figura ,  el  LLM  calcula  la  pérdida  de  entropía  cruzada  solo  para  los  identificadores  de  destino  de  respuesta  generados.  Al  ocultar  los  tokens  de  instrucción,  el  modelo  se  entrena  para  centrarse  en  generar  respuestas  precisas  en  lugar  de  memorizar  instrucciones,  lo  que  puede  ayudar  a  reducir  el  sobreajuste.\n",
    "\n",
    "Actualmente,  los  investigadores  no  están  de  acuerdo  sobre  si  enmascarar  las  instrucciones,  como  se  muestra  en  la  figura,  es  beneficioso  durante  el  ajuste  fino  de  instrucciones.\n",
    "\n",
    "[Creación de cargadores de datos para un cnjunto de datos de instrucciones](./4_cargadores_datos_instrucciones.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
