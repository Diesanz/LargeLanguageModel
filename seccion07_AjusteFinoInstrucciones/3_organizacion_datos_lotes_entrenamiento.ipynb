{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e841d3",
   "metadata": {},
   "source": [
    "#  Organización de datos en lotes de entrenamiento\n",
    "\n",
    "El siguiente paso es construir los lotes de entrenamiento de manera efectiva. Esto implica definir un método que garantice que el modelo reciba los datos de entrenamiento formateados durante el proceso de ajuste.\n",
    "\n",
    "![Texto alternativo](./imgs/7.5.png)\n",
    "\n",
    "En la sección 06,   los  lotes  de  entrenamiento  fueron  creados  automáticamente  por  la  clase  DataLoader  de  PyTorch ,  que  emplea  una  función  de  intercalación  predeterminada  para  combinar  listas  de  muestras  en  lotes.  Esta  función  se  encarga  de  tomar  una  lista  de muestras  de  datos  individuales  y  combinarlas  en  un  único  lote  que  el  modelo  puede  procesar  eficientemente  durante  el  entrenamiento.\n",
    "\n",
    "Sin  embargo,  el  proceso  de  procesamiento  por  lotes  para  el  ajuste  fino  de  instrucciones  en  esta sección es  un  poco  más  complejo  y  requiere  la  creación  de  una  función  de intercalación  personalizada  que  posteriormente se conectará  al  DataLoader.\n",
    "\n",
    "En  esta  sección,  se abordará  el  proceso  de  procesamiento  por  lotes  en  varios  pasos,  incluida  la  codificación de  la  función  de  intercalación  personalizada.\n",
    "\n",
    "![Texto alternativo](./imgs/7.6.png)\n",
    "\n",
    "Primero se implementan los pasos 2.1 y 2.2, se codifica una clase InstructionDataset que aplica format_input y pre_tokeniza todas las entradas en el conjunto de datos.\n",
    "\n",
    "![Texto alternativo](./imgs/7.7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95aeebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_text = []\n",
    "\n",
    "        for entry in data:                                       \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_text[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d27627",
   "metadata": {},
   "source": [
    "Similar al enfoque de la sección 06, se busca acelerar el entrenamiento recopilando múltiples ejemplos de entrenamiento en un lote, lo que requiere rellenar todas las entradas con una logitud similar.  En  lugar  de  añadir  los  tokens  <|endoftext|>  a  las  entradas  de  texto,  se puede añadir  su  ID  de  token  directamente  a  las  entradas  pretokenizadas.  \n",
    "\n",
    "En  la sección 06, se igualaba  la  longitud  de  todos  los  ejemplos  de  un  conjunto  de  datos.  Pasando  al  paso  2.3  de  la  figura, se adopta  un  enfoque  más  sofisticado:  desarrollar  una  función  de  intercalación  personalizada  que  se puede pasar  al  cargador  de  datos.  Esta  función  de  intercalación  personalizada  iguala  la  longitud  de  los  ejemplos  de  entrenamiento  de  cada  lote,  permitiendo  que  los  distintos  lotes  tengan  longitudes  diferentes.  Este  enfoque  minimiza  el  relleno  innecesario,  extendiendo  las  secuencias  solo  para  que  coincidan  con  la  más  larga  de  cada  lote,  no  con  todo  el  conjunto  de  datos.\n",
    "\n",
    "![Texto alternativo](./imgs/7.8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ea6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_1(batch, pad_token_id=50256, device='cpu'):\n",
    "    #Calculamos las longitudes originales de cada secuencia\n",
    "    batchs_lengths = [len(item) for item in batch]\n",
    "    #Determinamos la longitud máxima del batch (+1 para añadir el token final)\n",
    "    batch_max_length = max(batchs_lengths) + 1\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        item += [pad_token_id]\n",
    "        padded = item + [pad_token_id] * (batch_max_length - len(item))\n",
    "        #Creamos el tensor de entrada quitando el último token ([:-1])\n",
    "        #Esto se hace para alinear inputs y labels en el entrenamiento autoregresivo\n",
    "        #El modelo ve \"inputs\" y debe predecir el siguiente token (label)\n",
    "        #Por eso inputs = secuencia sin el último token\n",
    "        #y labels  = secuencia sin el primero\n",
    "        input = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(input)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90149496",
   "metadata": {},
   "source": [
    "Como se puede  ver  en  base  al  resultado  anterior,  todas  las  entradas  se  han  rellenado  hasta  la  longitud  dela  lista  de  entrada  más  larga,  inputs_1,  que  contiene  5  ID  de  token. Sin  embargo,  como  se vio en las secciones 05  y  06,  también  se necesitan  crear  lotes con  los  ID  de  token  de  destino,  correspondientes  al  lote  de  ID  de  entrada.  Estos  ID  de  destino, son  cruciales  porque  representan  lo  que quiere  que  el  modelo  genera y  lo  que  se necesita  durante  el  entrenamiento  para  calcular  la  pérdida  para  las  actualizaciones  de  peso.\n",
    "\n",
    "![Texto alternativo](./imgs/7.9.png)\n",
    "\n",
    "\n",
    "Similar  al  proceso  descrito  en la sección 05  para  el  preentrenamiento  de  un  LLM,  los  ID  de  los  tokens  de  destino  coinciden  con  los  de  entrada,  pero  se  desplazan  una  posición  a  la  derecha.  Esta  configuración,  como  se  muestra  en  la  figura,  permite  al  LLM  aprender  a  predecir  el  siguiente  token  en  una  secuencia.\n",
    "\n",
    "![Texto alternativo](./imgs/7.10.png)\n",
    "\n",
    "La  siguiente  función  de  intercalación  actualizada  genera  los  ID  de  token  de  destino,  a  partir  de  los  ID  de  token  de  entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae78dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_2(batch,pad_token_id=50256,device=\"cpu\"):\n",
    "    batchs_lengths = [len(item) for item in batch]\n",
    "    batch_max_length = max(batchs_lengths) + 1\n",
    "    print(batch_max_length)\n",
    "    inputs_lst, target_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        item += [pad_token_id]\n",
    "        padded = item + [pad_token_id] * (batch_max_length - len(item))\n",
    "        input = torch.tensor(padded[:-1]) #tensor de entradas\n",
    "        target = torch.tensor(padded[1:]) #tensor de salidas\n",
    "        inputs_lst.append(input)\n",
    "        target_lst.append(target)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    target_tensor = torch.stack(target_lst).to(device)\n",
    "    return inputs_tensor, target_tensor\n",
    "\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
